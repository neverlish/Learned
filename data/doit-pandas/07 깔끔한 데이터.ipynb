{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 열과 피벗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 넓은 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### melt 메서드 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             religion  <$10k  $10-20k  $20-30k  $30-40k  $40-50k  $50-75k  \\\n",
      "0            Agnostic     27       34       60       81       76      137   \n",
      "1             Atheist     12       27       37       52       35       70   \n",
      "2            Buddhist     27       21       30       34       33       58   \n",
      "3            Catholic    418      617      732      670      638     1116   \n",
      "4  Don’t know/refused     15       14       15       11       10       35   \n",
      "\n",
      "   $75-100k  $100-150k  >150k  Don't know/refused  \n",
      "0       122        109     84                  96  \n",
      "1        73         59     74                  76  \n",
      "2        62         39     53                  54  \n",
      "3       949        792    633                1489  \n",
      "4        21         17     18                 116  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pew = pd.read_csv('./data/pew.csv')\n",
    "\n",
    "print(pew.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   religion  <$10k  $10-20k  $20-30k  $30-40k  $40-50k\n",
      "0                  Agnostic     27       34       60       81       76\n",
      "1                   Atheist     12       27       37       52       35\n",
      "2                  Buddhist     27       21       30       34       33\n",
      "3                  Catholic    418      617      732      670      638\n",
      "4        Don’t know/refused     15       14       15       11       10\n",
      "5          Evangelical Prot    575      869     1064      982      881\n",
      "6                     Hindu      1        9        7        9       11\n",
      "7   Historically Black Prot    228      244      236      238      197\n",
      "8         Jehovah's Witness     20       27       24       24       21\n",
      "9                    Jewish     19       19       25       25       30\n",
      "10            Mainline Prot    289      495      619      655      651\n",
      "11                   Mormon     29       40       48       51       56\n",
      "12                   Muslim      6        7        9       10        9\n",
      "13                 Orthodox     13       17       23       32       32\n",
      "14          Other Christian      9        7       11       13       13\n",
      "15             Other Faiths     20       33       40       46       49\n",
      "16    Other World Religions      5        2        3        4        2\n",
      "17             Unaffiliated    217      299      374      365      341\n"
     ]
    }
   ],
   "source": [
    "print(pew.iloc[:, 0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             religion variable  value\n",
      "0            Agnostic    <$10k     27\n",
      "1             Atheist    <$10k     12\n",
      "2            Buddhist    <$10k     27\n",
      "3            Catholic    <$10k    418\n",
      "4  Don’t know/refused    <$10k     15\n"
     ]
    }
   ],
   "source": [
    "pew_long = pd.melt(pew, id_vars='religion')\n",
    "print(pew_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             religion income  count\n",
      "0            Agnostic  <$10k     27\n",
      "1             Atheist  <$10k     12\n",
      "2            Buddhist  <$10k     27\n",
      "3            Catholic  <$10k    418\n",
      "4  Don’t know/refused  <$10k     15\n"
     ]
    }
   ],
   "source": [
    "pew_long = pd.melt(pew, id_vars='religion', var_name='income', value_name='count')\n",
    "print(pew_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year        artist                    track  time date.entered  wk1   wk2  \\\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26   87  82.0   \n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02   91  87.0   \n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08   81  70.0   \n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21   76  76.0   \n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15   57  34.0   \n",
      "\n",
      "    wk3   wk4   wk5   wk6   wk7   wk8   wk9  wk10  wk11  \n",
      "0  72.0  77.0  87.0  94.0  99.0   NaN   NaN   NaN   NaN  \n",
      "1  92.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "2  68.0  67.0  66.0  57.0  54.0  53.0  51.0  51.0  51.0  \n",
      "3  72.0  69.0  67.0  65.0  55.0  59.0  62.0  61.0  61.0  \n",
      "4  25.0  17.0  17.0  31.0  36.0  49.0  53.0  57.0  64.0  \n"
     ]
    }
   ],
   "source": [
    "billboard = pd.read_csv('./data/billboard.csv')\n",
    "\n",
    "print(billboard.iloc[0:5, 0:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year        artist                    track  time date.entered week  rating\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk1    87.0\n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02  wk1    91.0\n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08  wk1    81.0\n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21  wk1    76.0\n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15  wk1    57.0\n"
     ]
    }
   ],
   "source": [
    "billboard_long = pd.melt(billboard, id_vars=['year', 'artist', 'track', 'time', 'date.entered'], var_name='week', value_name='rating')\n",
    "print(billboard_long.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 열 이름 관리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하나의 열이 여러 의미를 가지고 있는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ebola 데이터 집합 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Day', 'Cases_Guinea', 'Cases_Liberia', 'Cases_SierraLeone',\n",
      "       'Cases_Nigeria', 'Cases_Senegal', 'Cases_UnitedStates', 'Cases_Spain',\n",
      "       'Cases_Mali', 'Deaths_Guinea', 'Deaths_Liberia', 'Deaths_SierraLeone',\n",
      "       'Deaths_Nigeria', 'Deaths_Senegal', 'Deaths_UnitedStates',\n",
      "       'Deaths_Spain', 'Deaths_Mali'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "ebola = pd.read_csv('./data/country_timeseries.csv')\n",
    "\n",
    "print(ebola.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  Cases_Guinea  Cases_Liberia  Deaths_Liberia\n",
      "0    1/5/2015  289        2776.0            NaN             NaN\n",
      "1    1/4/2015  288        2775.0            NaN             NaN\n",
      "2    1/3/2015  287        2769.0         8166.0          3496.0\n",
      "3    1/2/2015  286           NaN         8157.0          3496.0\n",
      "4  12/31/2014  284        2730.0         8115.0          3471.0\n"
     ]
    }
   ],
   "source": [
    "print(ebola.iloc[:5, [0, 1, 2, 3, 11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day      variable   value\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0\n",
      "3    1/2/2015  286  Cases_Guinea     NaN\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0\n"
     ]
    }
   ],
   "source": [
    "ebola_long = pd.melt(ebola, id_vars=['Date', 'Day'])\n",
    "print(ebola_long.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split 메서드로 열 이름 분리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 열 이름 분리하고 데이터프레임에 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Cases, Guinea]\n",
      "1    [Cases, Guinea]\n",
      "2    [Cases, Guinea]\n",
      "3    [Cases, Guinea]\n",
      "4    [Cases, Guinea]\n",
      "Name: variable, dtype: object\n"
     ]
    }
   ],
   "source": [
    "variable_split = ebola_long.variable.str.split('_')\n",
    "print(variable_split[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(variable_split))\n",
    "print(type(variable_split[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_values = variable_split.str.get(0)\n",
    "country_values = variable_split.str.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Cases\n",
      "1    Cases\n",
      "2    Cases\n",
      "3    Cases\n",
      "4    Cases\n",
      "Name: variable, dtype: object\n",
      "1947    Deaths\n",
      "1948    Deaths\n",
      "1949    Deaths\n",
      "1950    Deaths\n",
      "1951    Deaths\n",
      "Name: variable, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(status_values[:5])\n",
    "print(status_values[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Guinea\n",
      "1    Guinea\n",
      "2    Guinea\n",
      "3    Guinea\n",
      "4    Guinea\n",
      "Name: variable, dtype: object\n",
      "1947    Mali\n",
      "1948    Mali\n",
      "1949    Mali\n",
      "1950    Mali\n",
      "1951    Mali\n",
      "Name: variable, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(country_values[:5])\n",
    "print(country_values[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day      variable   value status country\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0  Cases  Guinea\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0  Cases  Guinea\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0  Cases  Guinea\n",
      "3    1/2/2015  286  Cases_Guinea     NaN  Cases  Guinea\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "ebola_long['status'] = status_values\n",
    "ebola_long['country'] = country_values\n",
    "print(ebola_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day      variable   value status country status country\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0  Cases  Guinea  Cases  Guinea\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0  Cases  Guinea  Cases  Guinea\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0  Cases  Guinea  Cases  Guinea\n",
      "3    1/2/2015  286  Cases_Guinea     NaN  Cases  Guinea  Cases  Guinea\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0  Cases  Guinea  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "variable_split = ebola_long.variable.str.split('_', expand=True)\n",
    "variable_split.columns = ['status', 'country']\n",
    "ebola_parsed = pd.concat([ebola_long, variable_split], axis=1)\n",
    "\n",
    "print(ebola_parsed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 여러 열을 하나로 정리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기상 데이터의 여러 열을 하나로 정리하기 - melt, pivot_table 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  year  month element  d1    d2    d3  d4    d5  d6  ...  d22   d23  \\\n",
      "0  MX17004  2010      1    tmax NaN   NaN   NaN NaN   NaN NaN  ...  NaN   NaN   \n",
      "1  MX17004  2010      1    tmin NaN   NaN   NaN NaN   NaN NaN  ...  NaN   NaN   \n",
      "2  MX17004  2010      2    tmax NaN  27.3  24.1 NaN   NaN NaN  ...  NaN  29.9   \n",
      "3  MX17004  2010      2    tmin NaN  14.4  14.4 NaN   NaN NaN  ...  NaN  10.7   \n",
      "4  MX17004  2010      3    tmax NaN   NaN   NaN NaN  32.1 NaN  ...  NaN   NaN   \n",
      "\n",
      "   d24  d25  d26  d27  d28  d29   d30  d31  \n",
      "0  NaN  NaN  NaN  NaN  NaN  NaN  27.8  NaN  \n",
      "1  NaN  NaN  NaN  NaN  NaN  NaN  14.5  NaN  \n",
      "2  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  \n",
      "3  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  \n",
      "4  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "weather = pd.read_csv('./data/weather.csv')\n",
    "\n",
    "print(weather.iloc[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  year  month element day  temp\n",
      "0  MX17004  2010      1    tmax  d1   NaN\n",
      "1  MX17004  2010      1    tmin  d1   NaN\n",
      "2  MX17004  2010      2    tmax  d1   NaN\n",
      "3  MX17004  2010      2    tmin  d1   NaN\n",
      "4  MX17004  2010      3    tmax  d1   NaN\n"
     ]
    }
   ],
   "source": [
    "weather_melt = pd.melt(weather, id_vars=['id', 'year', 'month', 'element'], var_name='day', value_name='temp')\n",
    "print(weather_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element                 tmax  tmin\n",
      "id      year month day            \n",
      "MX17004 2010 1     d30  27.8  14.5\n",
      "             2     d11  29.7  13.4\n",
      "                   d2   27.3  14.4\n",
      "                   d23  29.9  10.7\n",
      "                   d3   24.1  14.4\n",
      "             3     d10  34.5  16.8\n",
      "                   d16  31.1  17.6\n",
      "                   d5   32.1  14.2\n",
      "             4     d27  36.3  16.7\n",
      "             5     d27  33.2  18.2\n",
      "             6     d17  28.0  17.5\n",
      "                   d29  30.1  18.0\n",
      "             7     d3   28.6  17.5\n",
      "                   d14  29.9  16.5\n",
      "             8     d23  26.4  15.0\n",
      "                   d5   29.6  15.8\n",
      "                   d29  28.0  15.3\n",
      "                   d13  29.8  16.5\n",
      "                   d25  29.7  15.6\n",
      "                   d31  25.4  15.4\n",
      "                   d8   29.0  17.3\n",
      "             10    d5   27.0  14.0\n",
      "                   d14  29.5  13.0\n",
      "                   d15  28.7  10.5\n",
      "                   d28  31.2  15.0\n",
      "                   d7   28.1  12.9\n",
      "             11    d2   31.3  16.3\n",
      "                   d5   26.3   7.9\n",
      "                   d27  27.7  14.2\n",
      "                   d26  28.1  12.1\n",
      "                   d4   27.2  12.0\n",
      "             12    d1   29.9  13.8\n",
      "                   d6   27.8  10.5\n"
     ]
    }
   ],
   "source": [
    "weather_tidy = weather_melt.pivot_table(\n",
    "    index=['id', 'year', 'month', 'day'],\n",
    "    columns='element',\n",
    "    values='temp'\n",
    ")\n",
    "\n",
    "print(weather_tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element       id  year  month  day  tmax  tmin\n",
      "0        MX17004  2010      1  d30  27.8  14.5\n",
      "1        MX17004  2010      2  d11  29.7  13.4\n",
      "2        MX17004  2010      2   d2  27.3  14.4\n",
      "3        MX17004  2010      2  d23  29.9  10.7\n",
      "4        MX17004  2010      2   d3  24.1  14.4\n"
     ]
    }
   ],
   "source": [
    "weather_tidy_flat = weather_tidy.reset_index()\n",
    "\n",
    "print(weather_tidy_flat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 중복 데이터 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 빌보드 차트의 중복 데이터 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24092, 7)\n",
      "   year        artist                    track  time date.entered week  rating\n",
      "0  2000         2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk1    87.0\n",
      "1  2000       2Ge+her  The Hardest Part Of ...  3:15   2000-09-02  wk1    91.0\n",
      "2  2000  3 Doors Down               Kryptonite  3:53   2000-04-08  wk1    81.0\n",
      "3  2000  3 Doors Down                    Loser  4:24   2000-10-21  wk1    76.0\n",
      "4  2000      504 Boyz            Wobble Wobble  3:35   2000-04-15  wk1    57.0\n"
     ]
    }
   ],
   "source": [
    "billboard = pd.read_csv('./data/billboard.csv')\n",
    "billboard_long = pd.melt(billboard, id_vars=['year', 'artist', 'track', 'time', 'date.entered'], var_name='week', value_name='rating')\n",
    "\n",
    "print(billboard_long.shape)\n",
    "\n",
    "print(billboard_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      year        artist  track  time date.entered week  rating\n",
      "3     2000  3 Doors Down  Loser  4:24   2000-10-21  wk1    76.0\n",
      "320   2000  3 Doors Down  Loser  4:24   2000-10-21  wk2    76.0\n",
      "637   2000  3 Doors Down  Loser  4:24   2000-10-21  wk3    72.0\n",
      "954   2000  3 Doors Down  Loser  4:24   2000-10-21  wk4    69.0\n",
      "1271  2000  3 Doors Down  Loser  4:24   2000-10-21  wk5    67.0\n"
     ]
    }
   ],
   "source": [
    "print(billboard_long[billboard_long.track == 'Loser'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24092, 4)\n"
     ]
    }
   ],
   "source": [
    "billboard_songs = billboard_long[['year', 'artist', 'track', 'time']]\n",
    "\n",
    "print(billboard_songs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317, 4)\n"
     ]
    }
   ],
   "source": [
    "billboard_songs = billboard_songs.drop_duplicates()\n",
    "\n",
    "print(billboard_songs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year          artist                    track  time  id\n",
      "0  2000           2 Pac  Baby Don't Cry (Keep...  4:22   0\n",
      "1  2000         2Ge+her  The Hardest Part Of ...  3:15   1\n",
      "2  2000    3 Doors Down               Kryptonite  3:53   2\n",
      "3  2000    3 Doors Down                    Loser  4:24   3\n",
      "4  2000        504 Boyz            Wobble Wobble  3:35   4\n",
      "5  2000            98^0  Give Me Just One Nig...  3:24   5\n",
      "6  2000         A*Teens            Dancing Queen  3:44   6\n",
      "7  2000         Aaliyah            I Don't Wanna  4:15   7\n",
      "8  2000         Aaliyah                Try Again  4:03   8\n",
      "9  2000  Adams, Yolanda            Open My Heart  5:30   9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "billboard_songs['id'] = range(len(billboard_songs))\n",
    "print(billboard_songs.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24092, 8)\n",
      "   year artist                    track  time date.entered week  rating  id\n",
      "0  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk1    87.0   0\n",
      "1  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk2    82.0   0\n",
      "2  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk3    72.0   0\n",
      "3  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk4    77.0   0\n",
      "4  2000  2 Pac  Baby Don't Cry (Keep...  4:22   2000-02-26  wk5    87.0   0\n"
     ]
    }
   ],
   "source": [
    "billboard_ratings = billboard_long.merge(billboard_songs, on=['year', 'artist', 'track', 'time'])\n",
    "\n",
    "print(billboard_ratings.shape)\n",
    "\n",
    "print(billboard_ratings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 대용량 데이터 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러 개로 나누어진 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 뉴욕 택시 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-01.csv\n",
      "\n",
      "./data/fhv_tripdata_2015-01.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-02.csv\n",
      "\n",
      "./data/fhv_tripdata_2015-02.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-03.csv\n",
      "\n",
      "./data/fhv_tripdata_2015-03.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-04.csv\n",
      "\n",
      "./data/fhv_tripdata_2015-04.csv\n",
      "https://s3.amazonaws.com/nyc-tlc/trip+data/fhv_tripdata_2015-05.csv\n",
      "\n",
      "./data/fhv_tripdata_2015-05.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "with open('./data/raw_data_urls.txt', 'r') as data_urls:\n",
    "    for line, url in enumerate(data_urls):\n",
    "        if line == 5:\n",
    "            break\n",
    "        fn = url.split('/')[-1].strip()\n",
    "        fp = os.path.join('', './data', fn)\n",
    "        print(url)\n",
    "        print(fp)\n",
    "        urllib.request.urlretrieve(url, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/fhv_tripdata_2015-05.csv', './data/fhv_tripdata_2015-04.csv', './data/fhv_tripdata_2015-01.csv', './data/fhv_tripdata_2015-03.csv', './data/fhv_tripdata_2015-02.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "nyc_taxi_data = glob.glob('./data/fhv_*')\n",
    "print(nyc_taxi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi1 = pd.read_csv(nyc_taxi_data[0])\n",
    "taxi2 = pd.read_csv(nyc_taxi_data[1])\n",
    "taxi3 = pd.read_csv(nyc_taxi_data[2])\n",
    "taxi4 = pd.read_csv(nyc_taxi_data[3])\n",
    "taxi5 = pd.read_csv(nyc_taxi_data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00001  2015-05-01 04:30:00         NaN\n",
      "1               B00001  2015-05-01 05:00:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00001  2015-04-01 04:30:00         NaN\n",
      "1               B00001  2015-04-01 06:00:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00013  2015-01-01 00:30:00         NaN\n",
      "1               B00013  2015-01-01 01:22:00         NaN\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00029  2015-03-01 00:02:00       213.0\n",
      "1               B00029  2015-03-01 00:03:00        51.0\n",
      "  Dispatching_base_num          Pickup_date  locationID\n",
      "0               B00013  2015-02-01 00:00:00         NaN\n",
      "1               B00013  2015-02-01 00:01:00         NaN\n"
     ]
    }
   ],
   "source": [
    "print(taxi1.head(n=2))\n",
    "print(taxi2.head(n=2))\n",
    "print(taxi3.head(n=2))\n",
    "print(taxi4.head(n=2))\n",
    "print(taxi5.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4296067, 3)\n",
      "(3917789, 3)\n",
      "(2746033, 3)\n",
      "(3281427, 3)\n",
      "(3126401, 3)\n"
     ]
    }
   ],
   "source": [
    "print(taxi1.shape)\n",
    "print(taxi2.shape)\n",
    "print(taxi3.shape)\n",
    "print(taxi4.shape)\n",
    "print(taxi5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17367717, 3)\n"
     ]
    }
   ],
   "source": [
    "taxi = pd.concat([taxi1, taxi2, taxi3, taxi4, taxi5])\n",
    "print(taxi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "list_taxi_df = []\n",
    "\n",
    "for csv_filename in nyc_taxi_data:\n",
    "    df = pd.read_csv(csv_filename)\n",
    "    list_taxi_df.append(df)\n",
    "    \n",
    "print(len(list_taxi_df))\n",
    "\n",
    "print(type(list_taxi_df[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17367717, 3)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "taxi_loop_concat = pd.concat(list_taxi_df)\n",
    "print(taxi_loop_concat.shape)\n",
    "\n",
    "print(taxi.equals(taxi_loop_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
