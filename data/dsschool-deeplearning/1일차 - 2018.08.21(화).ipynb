{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learing\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "### Regression\n",
    "\n",
    "- Linear/Logistic Regression\n",
    "- SVM\n",
    "- Artificial Neural Network(ANN) - Deep Learning\n",
    "\n",
    "### Tree\n",
    "\n",
    "- Decision Tree\n",
    "- Random Forest (+ Bagging)\n",
    "- GBMC (+Boosting)\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.60400253,  0.07075573,  0.69562433,  0.35100997,  0.67721794,\n",
       "        0.51425808,  0.25992823,  0.8636763 ,  0.67382303,  0.37800601])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Feature\n",
    "x = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "print(x.shape)\n",
    "x[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.18120076,  0.02122672,  0.2086873 ,  0.10530299,  0.20316538,\n",
       "        0.15427742,  0.07797847,  0.25910289,  0.20214691,  0.1134018 ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label\n",
    "y = 0.3 * x\n",
    "\n",
    "print(y.shape)\n",
    "y[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10d72d080>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZpYx/x9+Nr92NGTj/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1035755f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0, w = 0.319293, error = 0.009829\n",
      "26, w = 0.299511, error = 0.000249\n",
      "----------------------------------------\n",
      "26, w = 0.299511, error = 0.000249\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "best_error = 9999\n",
    "best_w = None\n",
    "best_epoch = None\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    w = np.random.uniform(low=0.0, high=1.0)\n",
    "    y_predict = w * x\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_w = w\n",
    "        best_epoch = epoch\n",
    "        \n",
    "        print(f\"{epoch:2}, w = {w:.6f}, error = {error:.6f}\")\n",
    "        \n",
    "print('----' * 10)\n",
    "print(f\"{best_epoch:2}, w = {best_w:.6f}, error = {best_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h-step Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w = 0.894090, error = 0.302648\n",
      " 1 w = 0.864090, error = 0.287365\n",
      " 2 w = 0.834090, error = 0.272082\n",
      " 3 w = 0.804090, error = 0.256799\n",
      " 4 w = 0.774090, error = 0.241516\n",
      " 5 w = 0.744090, error = 0.226234\n",
      " 6 w = 0.714090, error = 0.210951\n",
      " 7 w = 0.684090, error = 0.195668\n",
      " 8 w = 0.654090, error = 0.180385\n",
      " 9 w = 0.624090, error = 0.165102\n",
      "10 w = 0.594090, error = 0.149819\n",
      "11 w = 0.564090, error = 0.134536\n",
      "12 w = 0.534090, error = 0.119253\n",
      "13 w = 0.504090, error = 0.103970\n",
      "14 w = 0.474090, error = 0.088687\n",
      "15 w = 0.444090, error = 0.073404\n",
      "16 w = 0.414090, error = 0.058121\n",
      "17 w = 0.384090, error = 0.042838\n",
      "18 w = 0.354090, error = 0.027555\n",
      "19 w = 0.324090, error = 0.012272\n",
      "20 w = 0.294090, error = 0.003011\n",
      "21 w = 0.294090, error = 0.003011\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "h = 0.03\n",
    "\n",
    "w = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w * x\n",
    "    \n",
    "    current_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    y_predict = (w + h) * x\n",
    "    h_plus_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if h_plus_error < current_error:\n",
    "        w = w + h\n",
    "        \n",
    "        print(f\"{epoch:2} w = {w:.6f}, error = {h_plus_error:.6f}\")\n",
    "        continue\n",
    "        \n",
    "    y_predict = (w - h) * x\n",
    "    h_minus_error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if h_minus_error < current_error:\n",
    "        w = w - h\n",
    "        print(f\"{epoch:2} w = {w:.6f}, error = {h_minus_error:.6f}\")\n",
    "        continue\n",
    "    break\n",
    "    \n",
    "print(f\"{epoch:2} w = {w:.6f}, error = {current_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Descent(not yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 w = 0.510543, error = 0.218640\n",
      " 1 w = 0.403286, error = 0.107258\n",
      " 2 w = 0.350669, error = 0.052617\n",
      "----------------------------------------\n",
      " 3 w = 0.324856, error = 0.025812\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "w = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w * x\n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    w = w - (y_predict - y).mean()\n",
    "    \n",
    "    if error < 0.05:\n",
    "        break\n",
    "        \n",
    "    print(f\"{epoch:2} w = {w:.6f}, error = {error:.6f}\")\n",
    "    \n",
    "print('----' * 10)\n",
    "print(f\"{epoch:2} w = {w:.6f}, error = {error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-------\n",
    "\n",
    "## Generate Dataset - 2 factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.20507918,  0.57913602,  0.06536204,  0.26355649,  0.55902324,\n",
       "        0.1129681 ,  0.01637194,  0.6763658 ,  0.84607436,  0.84442787])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "print(x1.shape)\n",
    "x1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.19412196,  0.07769054,  0.36314842,  0.58720033,  0.67302214,\n",
       "        0.32299989,  0.54056216,  0.57671332,  0.36213889,  0.37452512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "print(x2.shape)\n",
    "x2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.15858473,  0.21258607,  0.20118282,  0.37266711,  0.50421804,\n",
       "        0.19539038,  0.27519266,  0.4912664 ,  0.43489175,  0.44059092])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0.3 * x1 + 0.5 * x2\n",
    "\n",
    "print(y.shape)\n",
    "y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0, w1 = 0.945032, w2 = 0.316146, error = 0.242433\n",
      " 1, w1 = 0.278087, w2 = 0.611331, error = 0.045003\n",
      " 2, w1 = 0.824798, w2 = 0.909579, error = 0.445228\n",
      " 3, w1 = 0.791908, w2 = 0.995558, error = 0.471032\n",
      " 4, w1 = 0.859315, w2 = 0.882019, error = 0.448296\n",
      " 5, w1 = 0.893678, w2 = 0.296496, error = 0.214700\n",
      " 6, w1 = 0.480633, w2 = 0.784369, error = 0.222223\n",
      " 7, w1 = 0.861347, w2 = 0.610824, error = 0.318803\n",
      " 8, w1 = 0.823303, w2 = 0.528865, error = 0.261385\n",
      " 9, w1 = 0.612434, w2 = 0.557683, error = 0.175516\n",
      "10, w1 = 0.536411, w2 = 0.548078, error = 0.134940\n",
      "11, w1 = 0.269159, w2 = 0.384083, error = 0.070347\n",
      "12, w1 = 0.906590, w2 = 0.406433, error = 0.250111\n",
      "13, w1 = 0.837972, w2 = 0.398292, error = 0.215755\n",
      "14, w1 = 0.841919, w2 = 0.668763, error = 0.337485\n",
      "15, w1 = 0.607760, w2 = 0.104364, error = 0.124480\n",
      "16, w1 = 0.307531, w2 = 0.832355, error = 0.163436\n",
      "17, w1 = 0.610090, w2 = 0.063523, error = 0.136498\n",
      "18, w1 = 0.585583, w2 = 0.339137, error = 0.092121\n",
      "19, w1 = 0.334645, w2 = 0.574570, error = 0.052256\n",
      "20, w1 = 0.367470, w2 = 0.024476, error = 0.201308\n",
      "21, w1 = 0.532785, w2 = 0.775218, error = 0.242486\n",
      "22, w1 = 0.847532, w2 = 0.303075, error = 0.195801\n",
      "23, w1 = 0.984904, w2 = 0.091298, error = 0.218173\n",
      "24, w1 = 0.950848, w2 = 0.450885, error = 0.286751\n",
      "25, w1 = 0.728040, w2 = 0.806396, error = 0.349832\n",
      "26, w1 = 0.209140, w2 = 0.608350, error = 0.034276\n",
      "27, w1 = 0.323949, w2 = 0.299479, error = 0.086627\n",
      "28, w1 = 0.321277, w2 = 0.770735, error = 0.140296\n",
      "29, w1 = 0.184389, w2 = 0.571397, error = 0.036543\n",
      "30, w1 = 0.312067, w2 = 0.226728, error = 0.126307\n",
      "31, w1 = 0.332509, w2 = 0.650192, error = 0.087623\n",
      "32, w1 = 0.358845, w2 = 0.687947, error = 0.118240\n",
      "33, w1 = 0.778613, w2 = 0.954781, error = 0.445129\n",
      "34, w1 = 0.630691, w2 = 0.136129, error = 0.116786\n",
      "35, w1 = 0.132512, w2 = 0.796205, error = 0.094393\n",
      "36, w1 = 0.918919, w2 = 0.070441, error = 0.191259\n",
      "37, w1 = 0.120438, w2 = 0.320756, error = 0.171147\n",
      "38, w1 = 0.392504, w2 = 0.713647, error = 0.146522\n",
      "39, w1 = 0.756232, w2 = 0.370081, error = 0.171503\n",
      "40, w1 = 0.045854, w2 = 0.872494, error = 0.116548\n",
      "41, w1 = 0.958815, w2 = 0.934357, error = 0.520532\n",
      "42, w1 = 0.557583, w2 = 0.711059, error = 0.223352\n",
      "43, w1 = 0.010269, w2 = 0.741154, error = 0.089740\n",
      "44, w1 = 0.950040, w2 = 0.091872, error = 0.204675\n",
      "45, w1 = 0.854645, w2 = 0.309982, error = 0.200610\n",
      "46, w1 = 0.750818, w2 = 0.818490, error = 0.366422\n",
      "47, w1 = 0.643955, w2 = 0.660315, error = 0.239793\n",
      "48, w1 = 0.643433, w2 = 0.625146, error = 0.222629\n",
      "49, w1 = 0.295911, w2 = 0.303554, error = 0.096431\n",
      "50, w1 = 0.737312, w2 = 0.933076, error = 0.415154\n",
      "51, w1 = 0.393917, w2 = 0.030053, error = 0.189510\n",
      "52, w1 = 0.398505, w2 = 0.049666, error = 0.178874\n",
      "53, w1 = 0.063990, w2 = 0.593879, error = 0.082539\n",
      "54, w1 = 0.273971, w2 = 0.464229, error = 0.029517\n",
      "55, w1 = 0.990294, w2 = 0.820866, error = 0.480827\n",
      "56, w1 = 0.907714, w2 = 0.973634, error = 0.515257\n",
      "57, w1 = 0.880531, w2 = 0.626989, error = 0.335653\n",
      "58, w1 = 0.374452, w2 = 0.128647, error = 0.149674\n",
      "59, w1 = 0.775852, w2 = 0.767871, error = 0.353913\n",
      "60, w1 = 0.707959, w2 = 0.031862, error = 0.148819\n",
      "61, w1 = 0.298897, w2 = 0.709653, error = 0.100339\n",
      "62, w1 = 0.620307, w2 = 0.368975, error = 0.111318\n",
      "63, w1 = 0.791190, w2 = 0.803795, error = 0.378448\n",
      "64, w1 = 0.476638, w2 = 0.080089, error = 0.143892\n",
      "65, w1 = 0.927242, w2 = 0.050038, error = 0.192989\n",
      "66, w1 = 0.753410, w2 = 0.397718, error = 0.177161\n",
      "67, w1 = 0.779990, w2 = 0.154028, error = 0.147605\n",
      "68, w1 = 0.696721, w2 = 0.737985, error = 0.302111\n",
      "69, w1 = 0.764971, w2 = 0.150208, error = 0.142555\n",
      "70, w1 = 0.137787, w2 = 0.103797, error = 0.267307\n",
      "71, w1 = 0.330730, w2 = 0.882086, error = 0.198330\n",
      "72, w1 = 0.727631, w2 = 0.383725, error = 0.162104\n",
      "73, w1 = 0.499988, w2 = 0.153220, error = 0.110208\n",
      "74, w1 = 0.208191, w2 = 0.429905, error = 0.077140\n",
      "75, w1 = 0.942887, w2 = 0.177113, error = 0.212843\n",
      "76, w1 = 0.349976, w2 = 0.130999, error = 0.157110\n",
      "77, w1 = 0.283531, w2 = 0.334714, error = 0.087298\n",
      "78, w1 = 0.415957, w2 = 0.983033, error = 0.287198\n",
      "79, w1 = 0.038734, w2 = 0.474127, error = 0.136014\n",
      "80, w1 = 0.118883, w2 = 0.894434, error = 0.131833\n",
      "81, w1 = 0.305385, w2 = 0.158776, error = 0.161703\n",
      "82, w1 = 0.362049, w2 = 0.546694, error = 0.051808\n",
      "83, w1 = 0.501369, w2 = 0.446659, error = 0.076681\n",
      "84, w1 = 0.174594, w2 = 0.669549, error = 0.053162\n",
      "85, w1 = 0.790393, w2 = 0.807576, error = 0.379890\n",
      "86, w1 = 0.155460, w2 = 0.412592, error = 0.110407\n",
      "87, w1 = 0.393891, w2 = 0.713807, error = 0.147255\n",
      "88, w1 = 0.482551, w2 = 0.647029, error = 0.157065\n",
      "89, w1 = 0.309575, w2 = 0.475435, error = 0.008600\n",
      "90, w1 = 0.272930, w2 = 0.869927, error = 0.166664\n",
      "91, w1 = 0.552042, w2 = 0.551427, error = 0.143943\n",
      "92, w1 = 0.658649, w2 = 0.538122, error = 0.187964\n",
      "93, w1 = 0.023571, w2 = 0.250718, error = 0.250652\n",
      "94, w1 = 0.013902, w2 = 0.971496, error = 0.148849\n",
      "95, w1 = 0.820459, w2 = 0.164747, error = 0.162897\n",
      "96, w1 = 0.103762, w2 = 0.406430, error = 0.137823\n",
      "97, w1 = 0.243945, w2 = 0.588146, error = 0.027681\n",
      "98, w1 = 0.126194, w2 = 0.497609, error = 0.083353\n",
      "99, w1 = 0.013896, w2 = 0.435285, error = 0.166445\n",
      "----------------------------------------\n",
      "89, w1 = 0.309575, w2 = 0.475435, error = 0.008600\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "best_error = 9999\n",
    "best_w1 = None\n",
    "best_w2 = None\n",
    "best_epoch = None\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "    y_predict = w1 * x1 + w2 * x2\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_w1 = w1\n",
    "        best_w2 = w2\n",
    "        best_epoch = epoch\n",
    "        \n",
    "    print(f\"{epoch:2}, w1 = {w1:.6f}, w2 = {w2:.6f}, error = {error:.6f}\")\n",
    "    \n",
    "print('----' * 10)\n",
    "print(f\"{best_epoch:2}, w1 = {best_w1:.6f}, w2 = {best_w2:.6f}, error = {best_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent(not yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0, w1 = 0.304822, w2 = 0.664903, error = 0.172329\n",
      " 1, w1 = 0.266743, w2 = 0.613438, error = 0.081605\n",
      "----------------------------------------\n",
      "89, w1 = 0.309575, w2 = 0.475435, error = 0.008600\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w1 * x1 + w2 * x2\n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if error < 0.05:\n",
    "        break\n",
    "    \n",
    "    w1 = w1 - ((y_predict - y) * x1).mean()\n",
    "    w2 = w2 - ((y_predict - y) * x2).mean()\n",
    "        \n",
    "    print(f\"{epoch:2}, w1 = {w1:.6f}, w2 = {w2:.6f}, error = {error:.6f}\")\n",
    "    \n",
    "print('----' * 10)\n",
    "print(f\"{best_epoch:2}, w1 = {best_w1:.6f}, w2 = {best_w2:.6f}, error = {best_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Generate Dataset - 2 factors + 1 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.49200526,  0.66501136,  0.48141751,  0.14643528,  0.90404957,\n",
       "        0.80928262,  0.95430028,  0.45938196,  0.982184  ,  0.05141254])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "print(x1.shape)\n",
    "x1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.52722281,  0.3566    ,  0.02344097,  0.89033687,  0.81784016,\n",
       "        0.09958839,  0.76972724,  0.09487807,  0.39102675,  0.11220865])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.random.uniform(low=0.0, high=1.0, size=100)\n",
    "\n",
    "print(x2.shape)\n",
    "x2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.51121298,  0.47780341,  0.25614574,  0.58909902,  0.78013495,\n",
       "        0.39257898,  0.7711537 ,  0.28525362,  0.59016857,  0.17152809])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0.3 * x1 + 0.5 * x2 + 0.1\n",
    "\n",
    "print(y.shape)\n",
    "y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0, w1 = 0.222928, w2 = 0.762226, b: 0.889181 error = 0.884279\n",
      " 1, w1 = 0.502898, w2 = 0.846876, b: 0.005991 error = 0.183052\n",
      " 2, w1 = 0.366304, w2 = 0.662609, b: 0.430829 error = 0.444885\n",
      " 3, w1 = 0.575031, w2 = 0.399384, b: 0.275429 error = 0.257446\n",
      " 4, w1 = 0.774872, w2 = 0.705151, b: 0.952762 error = 1.185536\n",
      " 5, w1 = 0.330703, w2 = 0.513458, b: 0.842196 error = 0.763809\n",
      " 6, w1 = 0.282553, w2 = 0.826085, b: 0.955139 error = 1.011243\n",
      " 7, w1 = 0.181242, w2 = 0.474706, b: 0.667795 error = 0.497698\n",
      " 8, w1 = 0.724231, w2 = 0.117998, b: 0.319041 error = 0.241776\n",
      " 9, w1 = 0.849419, w2 = 0.125424, b: 0.981324 error = 0.957585\n",
      "10, w1 = 0.007311, w2 = 0.952601, b: 0.714961 error = 0.702015\n",
      "11, w1 = 0.610625, w2 = 0.752152, b: 0.802840 error = 0.980031\n",
      "12, w1 = 0.953276, w2 = 0.262124, b: 0.924459 error = 1.019834\n",
      "13, w1 = 0.105157, w2 = 0.725975, b: 0.216075 error = 0.138855\n",
      "14, w1 = 0.044297, w2 = 0.905042, b: 0.324513 error = 0.305527\n",
      "15, w1 = 0.483860, w2 = 0.356779, b: 0.210127 error = 0.126910\n",
      "16, w1 = 0.585690, w2 = 0.027460, b: 0.430031 error = 0.237721\n",
      "17, w1 = 0.602791, w2 = 0.258380, b: 0.863823 error = 0.788098\n",
      "18, w1 = 0.037708, w2 = 0.728663, b: 0.615169 error = 0.503910\n",
      "19, w1 = 0.238136, w2 = 0.409702, b: 0.444763 error = 0.269335\n",
      "20, w1 = 0.473065, w2 = 0.199191, b: 0.655498 error = 0.487278\n",
      "21, w1 = 0.822657, w2 = 0.562715, b: 0.779822 error = 0.963800\n",
      "22, w1 = 0.839979, w2 = 0.998659, b: 0.925380 error = 1.337678\n",
      "23, w1 = 0.414644, w2 = 0.699357, b: 0.189226 error = 0.245162\n",
      "24, w1 = 0.866802, w2 = 0.400771, b: 0.066132 error = 0.208184\n",
      "25, w1 = 0.386552, w2 = 0.797924, b: 0.498186 error = 0.590291\n",
      "26, w1 = 0.771755, w2 = 0.298531, b: 0.047682 error = 0.137987\n",
      "27, w1 = 0.855893, w2 = 0.869118, b: 0.024380 error = 0.379943\n",
      "28, w1 = 0.029860, w2 = 0.555840, b: 0.739304 error = 0.537057\n",
      "29, w1 = 0.883008, w2 = 0.328076, b: 0.673830 error = 0.768556\n",
      "30, w1 = 0.143332, w2 = 0.211143, b: 0.418806 error = 0.111378\n",
      "31, w1 = 0.998600, w2 = 0.677517, b: 0.002881 error = 0.333524\n",
      "32, w1 = 0.890855, w2 = 0.338217, b: 0.169812 error = 0.277469\n",
      "33, w1 = 0.369539, w2 = 0.688950, b: 0.705758 error = 0.734667\n",
      "34, w1 = 0.417940, w2 = 0.057087, b: 0.624598 error = 0.358065\n",
      "35, w1 = 0.074529, w2 = 0.343015, b: 0.956992 error = 0.668929\n",
      "36, w1 = 0.230275, w2 = 0.222994, b: 0.781429 error = 0.508002\n",
      "37, w1 = 0.116961, w2 = 0.817429, b: 0.131909 error = 0.124593\n",
      "38, w1 = 0.323011, w2 = 0.916403, b: 0.400683 error = 0.521890\n",
      "39, w1 = 0.743026, w2 = 0.729593, b: 0.265833 error = 0.495564\n",
      "40, w1 = 0.725701, w2 = 0.530914, b: 0.791000 error = 0.912122\n",
      "41, w1 = 0.893645, w2 = 0.214581, b: 0.450332 error = 0.492929\n",
      "42, w1 = 0.205258, w2 = 0.756111, b: 0.014579 error = 0.068300\n",
      "43, w1 = 0.820329, w2 = 0.534498, b: 0.136939 error = 0.305556\n",
      "44, w1 = 0.450963, w2 = 0.904105, b: 0.461249 error = 0.638025\n",
      "45, w1 = 0.438693, w2 = 0.342075, b: 0.037312 error = 0.081390\n",
      "46, w1 = 0.691221, w2 = 0.666345, b: 0.105615 error = 0.278423\n",
      "47, w1 = 0.890507, w2 = 0.055556, b: 0.011901 error = 0.174001\n",
      "48, w1 = 0.869196, w2 = 0.361830, b: 0.252567 error = 0.357656\n",
      "49, w1 = 0.989386, w2 = 0.334876, b: 0.005541 error = 0.212644\n",
      "50, w1 = 0.135873, w2 = 0.187195, b: 0.188215 error = 0.153290\n",
      "51, w1 = 0.952691, w2 = 0.053275, b: 0.718750 error = 0.708467\n",
      "52, w1 = 0.821947, w2 = 0.700085, b: 0.154256 error = 0.407201\n",
      "53, w1 = 0.713999, w2 = 0.466795, b: 0.471433 error = 0.554554\n",
      "54, w1 = 0.568899, w2 = 0.476269, b: 0.240729 error = 0.258577\n",
      "55, w1 = 0.226693, w2 = 0.487497, b: 0.941681 error = 0.799981\n",
      "56, w1 = 0.988146, w2 = 0.087416, b: 0.687610 error = 0.711670\n",
      "57, w1 = 0.588757, w2 = 0.500148, b: 0.879365 error = 0.918849\n",
      "58, w1 = 0.884840, w2 = 0.297494, b: 0.427600 error = 0.507780\n",
      "59, w1 = 0.602718, w2 = 0.464071, b: 0.883146 error = 0.911168\n",
      "60, w1 = 0.985906, w2 = 0.185552, b: 0.766123 error = 0.838617\n",
      "61, w1 = 0.666124, w2 = 0.816741, b: 0.280159 error = 0.516734\n",
      "62, w1 = 0.091371, w2 = 0.833390, b: 0.764862 error = 0.732352\n",
      "63, w1 = 0.342844, w2 = 0.644346, b: 0.923762 error = 0.917278\n",
      "64, w1 = 0.247978, w2 = 0.252355, b: 0.472584 error = 0.222518\n",
      "65, w1 = 0.126853, w2 = 0.414849, b: 0.140147 error = 0.088234\n",
      "66, w1 = 0.694855, w2 = 0.724658, b: 0.164353 error = 0.368338\n",
      "67, w1 = 0.386745, w2 = 0.414457, b: 0.316451 error = 0.215170\n",
      "68, w1 = 0.623555, w2 = 0.620201, b: 0.610272 error = 0.727130\n",
      "69, w1 = 0.237784, w2 = 0.009475, b: 0.701997 error = 0.324463\n",
      "70, w1 = 0.957766, w2 = 0.927927, b: 0.970812 error = 1.404288\n",
      "71, w1 = 0.296992, w2 = 0.747371, b: 0.232941 error = 0.256300\n",
      "72, w1 = 0.973403, w2 = 0.323371, b: 0.480173 error = 0.616167\n",
      "73, w1 = 0.114304, w2 = 0.397064, b: 0.402048 error = 0.160459\n",
      "74, w1 = 0.845419, w2 = 0.354055, b: 0.249895 error = 0.339581\n",
      "75, w1 = 0.528554, w2 = 0.207541, b: 0.752470 error = 0.615252\n",
      "76, w1 = 0.095788, w2 = 0.115228, b: 0.760374 error = 0.367644\n",
      "77, w1 = 0.930930, w2 = 0.407520, b: 0.742087 error = 0.900033\n",
      "78, w1 = 0.832813, w2 = 0.087850, b: 0.932671 error = 0.881957\n",
      "79, w1 = 0.289264, w2 = 0.561482, b: 0.923899 error = 0.849737\n",
      "80, w1 = 0.060475, w2 = 0.675604, b: 0.152936 error = 0.071901\n",
      "81, w1 = 0.558691, w2 = 0.160969, b: 0.244359 error = 0.128177\n",
      "82, w1 = 0.890668, w2 = 0.383813, b: 0.524652 error = 0.651199\n",
      "83, w1 = 0.965610, w2 = 0.750923, b: 0.222379 error = 0.570334\n",
      "84, w1 = 0.829667, w2 = 0.454764, b: 0.777917 error = 0.910812\n",
      "85, w1 = 0.706890, w2 = 0.562478, b: 0.511537 error = 0.639503\n",
      "86, w1 = 0.500890, w2 = 0.035021, b: 0.305956 error = 0.132913\n",
      "87, w1 = 0.702256, w2 = 0.070664, b: 0.807999 error = 0.685582\n",
      "88, w1 = 0.393958, w2 = 0.823996, b: 0.043118 error = 0.154158\n",
      "89, w1 = 0.945166, w2 = 0.108095, b: 0.942644 error = 0.956388\n",
      "90, w1 = 0.877160, w2 = 0.551119, b: 0.459710 error = 0.664150\n",
      "91, w1 = 0.723825, w2 = 0.936991, b: 0.795301 error = 1.120406\n",
      "92, w1 = 0.265252, w2 = 0.328793, b: 0.178161 error = 0.049488\n",
      "93, w1 = 0.018328, w2 = 0.270278, b: 0.738435 error = 0.386539\n",
      "94, w1 = 0.931516, w2 = 0.536003, b: 0.284849 error = 0.507905\n",
      "95, w1 = 0.724130, w2 = 0.523148, b: 0.568462 error = 0.684908\n",
      "96, w1 = 0.672213, w2 = 0.013468, b: 0.459161 error = 0.297020\n",
      "97, w1 = 0.713215, w2 = 0.587638, b: 0.388044 error = 0.531758\n",
      "98, w1 = 0.301296, w2 = 0.888111, b: 0.721425 error = 0.817874\n",
      "99, w1 = 0.331890, w2 = 0.023537, b: 0.683185 error = 0.358180\n",
      "----------------------------------------\n",
      "92, w1 = 0.265252, w2 = 0.328793, b = 0.178161, error = 0.049488\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "best_error = 9999\n",
    "best_w1 = None\n",
    "best_w2 = None\n",
    "best_b = None\n",
    "best_epoch = None\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "    w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "    b = np.random.uniform(low=0.0, high=1.0)\n",
    "    \n",
    "    y_predict = w1 * x1 + w2 * x2 + b\n",
    "    \n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_w1 = w1\n",
    "        best_w2 = w2\n",
    "        best_b = b\n",
    "        best_epoch = epoch\n",
    "        \n",
    "    print(f\"{epoch:2}, w1 = {w1:.6f}, w2 = {w2:.6f}, b: {b:.6f} error = {error:.6f}\")\n",
    "    \n",
    "print('----' * 10)\n",
    "print(f\"{best_epoch:2}, w1 = {best_w1:.6f}, w2 = {best_w2:.6f}, b = {best_b:.6f}, error = {best_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent(not yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0, w1 = 0.220519, w2 = 0.390435, b = -0.070469, error = 0.513079\n",
      " 1, w1 = 0.354122, w2 = 0.533221, b = 0.193654, error = 0.264123\n",
      " 2, w1 = 0.284025, w2 = 0.461460, b = 0.057108, error = 0.136546\n",
      " 3, w1 = 0.319057, w2 = 0.500162, b = 0.127158, error = 0.070050\n",
      " 4, w1 = 0.299986, w2 = 0.481775, b = 0.090718, error = 0.036440\n",
      " 5, w1 = 0.308899, w2 = 0.492694, b = 0.109202, error = 0.018484\n",
      " 6, w1 = 0.303466, w2 = 0.488388, b = 0.099390, error = 0.009812\n",
      " 7, w1 = 0.305505, w2 = 0.491825, b = 0.104185, error = 0.005010\n",
      "----------------------------------------\n",
      " 8, w1 = 0.265252, w2 = 0.328793, b = 0.178161, error = 0.049488\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "b = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w1 * x1 + w2 * x2 + b\n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if error < 0.005:\n",
    "        break\n",
    "    \n",
    "    w1 = w1 - ((y_predict - y) * x1).mean()\n",
    "    w2 = w2 - ((y_predict - y) * x2).mean()\n",
    "    b = b - (y_predict - y).mean() # y = 0.3 * x1 + 0.5 * x2 + 0.1 * (1.0) 이라고 가정\n",
    "    \n",
    "    print(f\"{epoch:2}, w1 = {w1:.6f}, w2 = {w2:.6f}, b = {b:.6f}, error = {error:.6f}\")\n",
    "    \n",
    "print('----' * 10)\n",
    "print(f\"{epoch:2}, w1 = {best_w1:.6f}, w2 = {best_w2:.6f}, b = {best_b:.6f}, error = {best_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0, w1 = 0.609965, w2 = 0.497937, b = 0.376940, error = 1.080176\n",
      " 1, w1 = 0.518170, w2 = 0.412211, b = 0.206721, error = 0.425548\n",
      " 2, w1 = 0.478982, w2 = 0.381484, b = 0.139618, error = 0.167757\n",
      " 3, w1 = 0.460607, w2 = 0.372310, b = 0.113126, error = 0.075934\n",
      " 4, w1 = 0.450523, w2 = 0.371524, b = 0.102630, error = 0.052355\n",
      " 5, w1 = 0.443795, w2 = 0.373943, b = 0.098439, error = 0.046832\n",
      " 6, w1 = 0.438477, w2 = 0.377531, b = 0.096735, error = 0.044728\n",
      " 7, w1 = 0.433800, w2 = 0.381488, b = 0.096016, error = 0.043218\n",
      " 8, w1 = 0.429459, w2 = 0.385504, b = 0.095689, error = 0.041798\n",
      " 9, w1 = 0.425331, w2 = 0.389458, b = 0.095520, error = 0.040424\n",
      "10, w1 = 0.421363, w2 = 0.393306, b = 0.095418, error = 0.039092\n",
      "11, w1 = 0.417535, w2 = 0.397034, b = 0.095347, error = 0.037803\n",
      "12, w1 = 0.413833, w2 = 0.400639, b = 0.095291, error = 0.036557\n",
      "13, w1 = 0.410253, w2 = 0.404121, b = 0.095245, error = 0.035352\n",
      "14, w1 = 0.406787, w2 = 0.407484, b = 0.095206, error = 0.034186\n",
      "15, w1 = 0.403434, w2 = 0.410732, b = 0.095173, error = 0.033059\n",
      "16, w1 = 0.400187, w2 = 0.413868, b = 0.095145, error = 0.031969\n",
      "17, w1 = 0.397045, w2 = 0.416896, b = 0.095123, error = 0.030916\n",
      "18, w1 = 0.394003, w2 = 0.419819, b = 0.095105, error = 0.029897\n",
      "19, w1 = 0.391059, w2 = 0.422642, b = 0.095091, error = 0.028912\n",
      "20, w1 = 0.388208, w2 = 0.425368, b = 0.095082, error = 0.027960\n",
      "21, w1 = 0.385449, w2 = 0.427999, b = 0.095077, error = 0.027039\n",
      "22, w1 = 0.382778, w2 = 0.430540, b = 0.095076, error = 0.026148\n",
      "23, w1 = 0.380192, w2 = 0.432992, b = 0.095078, error = 0.025288\n",
      "24, w1 = 0.377689, w2 = 0.435360, b = 0.095084, error = 0.024455\n",
      "25, w1 = 0.375266, w2 = 0.437646, b = 0.095093, error = 0.023650\n",
      "26, w1 = 0.372920, w2 = 0.439853, b = 0.095105, error = 0.022872\n",
      "27, w1 = 0.370648, w2 = 0.441984, b = 0.095120, error = 0.022121\n",
      "28, w1 = 0.368449, w2 = 0.444041, b = 0.095137, error = 0.021395\n",
      "29, w1 = 0.366320, w2 = 0.446027, b = 0.095158, error = 0.020694\n",
      "30, w1 = 0.364259, w2 = 0.447943, b = 0.095180, error = 0.020015\n",
      "31, w1 = 0.362263, w2 = 0.449794, b = 0.095205, error = 0.019360\n",
      "32, w1 = 0.360331, w2 = 0.451580, b = 0.095231, error = 0.018725\n",
      "33, w1 = 0.358460, w2 = 0.453304, b = 0.095260, error = 0.018112\n",
      "34, w1 = 0.356649, w2 = 0.454968, b = 0.095291, error = 0.017519\n",
      "35, w1 = 0.354895, w2 = 0.456575, b = 0.095323, error = 0.016945\n",
      "36, w1 = 0.353196, w2 = 0.458126, b = 0.095357, error = 0.016391\n",
      "37, w1 = 0.351552, w2 = 0.459622, b = 0.095392, error = 0.015854\n",
      "38, w1 = 0.349959, w2 = 0.461067, b = 0.095429, error = 0.015336\n",
      "39, w1 = 0.348417, w2 = 0.462461, b = 0.095467, error = 0.014834\n",
      "40, w1 = 0.346924, w2 = 0.463807, b = 0.095506, error = 0.014349\n",
      "41, w1 = 0.345478, w2 = 0.465106, b = 0.095547, error = 0.013880\n",
      "42, w1 = 0.344077, w2 = 0.466359, b = 0.095588, error = 0.013426\n",
      "43, w1 = 0.342721, w2 = 0.467569, b = 0.095630, error = 0.012987\n",
      "44, w1 = 0.341408, w2 = 0.468736, b = 0.095673, error = 0.012564\n",
      "45, w1 = 0.340136, w2 = 0.469863, b = 0.095717, error = 0.012155\n",
      "46, w1 = 0.338904, w2 = 0.470950, b = 0.095762, error = 0.011759\n",
      "47, w1 = 0.337711, w2 = 0.472000, b = 0.095807, error = 0.011377\n",
      "48, w1 = 0.336556, w2 = 0.473012, b = 0.095852, error = 0.011007\n",
      "49, w1 = 0.335437, w2 = 0.473989, b = 0.095899, error = 0.010649\n",
      "50, w1 = 0.334353, w2 = 0.474932, b = 0.095945, error = 0.010303\n",
      "51, w1 = 0.333303, w2 = 0.475842, b = 0.095992, error = 0.009968\n",
      "52, w1 = 0.332286, w2 = 0.476720, b = 0.096040, error = 0.009644\n",
      "53, w1 = 0.331301, w2 = 0.477567, b = 0.096087, error = 0.009331\n",
      "54, w1 = 0.330347, w2 = 0.478384, b = 0.096135, error = 0.009028\n",
      "55, w1 = 0.329423, w2 = 0.479172, b = 0.096183, error = 0.008735\n",
      "56, w1 = 0.328528, w2 = 0.479933, b = 0.096231, error = 0.008451\n",
      "57, w1 = 0.327661, w2 = 0.480667, b = 0.096279, error = 0.008177\n",
      "58, w1 = 0.326821, w2 = 0.481375, b = 0.096328, error = 0.007912\n",
      "59, w1 = 0.326007, w2 = 0.482058, b = 0.096376, error = 0.007656\n",
      "60, w1 = 0.325219, w2 = 0.482717, b = 0.096424, error = 0.007408\n",
      "61, w1 = 0.324455, w2 = 0.483352, b = 0.096473, error = 0.007168\n",
      "62, w1 = 0.323715, w2 = 0.483965, b = 0.096521, error = 0.006935\n",
      "63, w1 = 0.322998, w2 = 0.484557, b = 0.096569, error = 0.006711\n",
      "64, w1 = 0.322304, w2 = 0.485127, b = 0.096617, error = 0.006494\n",
      "65, w1 = 0.321631, w2 = 0.485678, b = 0.096664, error = 0.006284\n",
      "66, w1 = 0.320979, w2 = 0.486208, b = 0.096712, error = 0.006081\n",
      "67, w1 = 0.320347, w2 = 0.486720, b = 0.096759, error = 0.005884\n",
      "68, w1 = 0.319735, w2 = 0.487214, b = 0.096806, error = 0.005694\n",
      "69, w1 = 0.319142, w2 = 0.487690, b = 0.096853, error = 0.005510\n",
      "70, w1 = 0.318567, w2 = 0.488149, b = 0.096900, error = 0.005333\n",
      "71, w1 = 0.318010, w2 = 0.488592, b = 0.096946, error = 0.005162\n",
      "----------------------------------------\n",
      "72, w1 = 0.265252, w2 = 0.328793, b = 0.178161, error = 0.049488\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "learning_rate = 0.4\n",
    "\n",
    "w1 = np.random.uniform(low=0.0, high=1.0)\n",
    "w2 = np.random.uniform(low=0.0, high=1.0)\n",
    "b = np.random.uniform(low=0.0, high=1.0)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = w1 * x1 + w2 * x2 + b\n",
    "    error = np.abs(y_predict - y).mean()\n",
    "    \n",
    "    if error < 0.005:\n",
    "        break\n",
    "    \n",
    "    w1 = w1 - learning_rate * ((y_predict - y) * x1).mean()\n",
    "    w2 = w2 - learning_rate * ((y_predict - y) * x2).mean()\n",
    "    b = b - learning_rate * (y_predict - y).mean() # y = 0.3 * x1 + 0.5 * x2 + 0.1 * (1.0) 이라고 가정\n",
    "    \n",
    "    print(f\"{epoch:2}, w1 = {w1:.6f}, w2 = {w2:.6f}, b = {b:.6f}, error = {error:.6f}\")\n",
    "    \n",
    "print('----' * 10)\n",
    "print(f\"{epoch:2}, w1 = {best_w1:.6f}, w2 = {best_w2:.6f}, b = {best_b:.6f}, error = {best_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Single-Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = feature\n",
    "y = label\n",
    "m = # of data\n",
    "\n",
    "w = weight\n",
    "b = bias\n",
    "\n",
    "h(x) = w*x + b\n",
    "\n",
    "Loss Function\n",
    "L(y, h(x)) = (h(x) - y)^2) / 2\n",
    "\n",
    "Const Function\n",
    "J(w, b) = ( Sigma(1~100) L(y[i], h(x[i]) ) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
