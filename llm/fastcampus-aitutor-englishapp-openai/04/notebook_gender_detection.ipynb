{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d029e-51ec-49f8-aa43-fa2f2035c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc738e45-1c75-47c7-90e9-edb2378a299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348bc6b7-0e8a-4efc-bfda-3cd0a7177f69",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f9e48-b80d-4e5b-ad3c-e3a42b4f8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLanguageDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the audio files.\n",
    "        \"\"\"\n",
    "        self.audio_db = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_db)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_path = os.path.join(self.audio_db.iloc[idx, 1])\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        gender = self.audio_db.iloc[idx, 4]\n",
    "\n",
    "        sample = (waveform, sample_rate, gender)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf87e60-0312-4ecc-afb8-e0d37a328ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = AudioLanguageDataset(csv_file='/home/wonkyum/fc-asr/gender/train.csv')\n",
    "dev_set = AudioLanguageDataset(csv_file='/home/wonkyum/fc-asr/gender/dev.csv')\n",
    "test_set = AudioLanguageDataset(csv_file='/home/wonkyum/fc-asr/gender/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f200c1-3cee-4db3-bdc4-a8caa0a5dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate, label = train_set[0]\n",
    "print(waveform)\n",
    "print(sample_rate)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502bbd5-3f14-4d99-be12-2d0964d50696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(waveform.t().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e6fb7-a75d-497c-9d4e-3b9d9f404024",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_first, _, gender_first = train_set[3]\n",
    "print(gender_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1fe5bc-0ff5-4a4c-9239-2695df7bb892",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(waveform_first.numpy(), rate=sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2977f59-dddb-4532-8ade-84c01e8f0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_second, _, gender_second = train_set[5000]\n",
    "print(gender_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dbfcff-5563-4d19-bb98-ae9b48674487",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(waveform_second.numpy(), rate=sample_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11941fe-028f-4a54-bebe-6b5b59d92766",
   "metadata": {},
   "source": [
    "# Formatting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717f3d3-507f-4bbf-a0c7-c02e2bfb61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [ \"male\", \"female\" ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1198a71c-8271-493d-abff-a6efea86985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39c754-ace8-48f8-b0b7-e944f99adaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    # Return the position of the word in labels\n",
    "    return torch.tensor(labels.index(word))\n",
    "\n",
    "\n",
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f246a0-796b-465b-b7e4-c85910e65061",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_to_index('male'))\n",
    "print(index_to_label(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de27ec-349b-4710-9961-433c9c2151ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, _, label, *_ in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e84d7f-74a5-4b9d-9b91-5c0eb51ede7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_decibel = 2 * 20 * math.log10(torch.iinfo(torch.int16).max)\n",
    "_gain = pow(10, 0.05 * _decibel)\n",
    "\n",
    "def _piecewise_linear_log(x):\n",
    "    x = x * _gain\n",
    "    x[x > math.e] = torch.log(x[x > math.e])\n",
    "    x[x <= math.e] = x[x <= math.e] / math.e\n",
    "    return x\n",
    "\n",
    "class FunctionalModule(torch.nn.Module):\n",
    "    def __init__(self, functional):\n",
    "        super().__init__()\n",
    "        self.functional = functional\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.functional(input)\n",
    "\n",
    "\n",
    "class GlobalStatsNormalization(torch.nn.Module):\n",
    "    def __init__(self, global_stats_path):\n",
    "        super().__init__()\n",
    "\n",
    "        with open(global_stats_path) as f:\n",
    "            blob = json.loads(f.read())\n",
    "\n",
    "        self.mean = torch.tensor(blob[\"mean\"])\n",
    "        self.invstddev = torch.tensor(blob[\"invstddev\"])\n",
    "\n",
    "    def forward(self, input):\n",
    "        return (input - self.mean) * self.invstddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386500e-fdfe-4d41-9a53-5ef2e3d522f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_decibel = 2 * 20 * math.log10(torch.iinfo(torch.int16).max)\n",
    "_gain = pow(10, 0.05 * _decibel)\n",
    "\n",
    "\n",
    "class TransfomrFeature(torch.nn.Module):\n",
    "    def __init__(self, global_stats_path):\n",
    "        super().__init__()\n",
    "        self._extra_pipeline = torch.nn.Sequential(\n",
    "            FunctionalModule(_piecewise_linear_log),\n",
    "            GlobalStatsNormalization(global_stats_path),\n",
    "        )\n",
    "        self._spectrogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=400, n_mels=80, hop_length=160)\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        mel_features = self._spectrogram_transform(waveform).squeeze(1).transpose(2, 1)\n",
    "        feats = self._extra_pipeline(mel_features)\n",
    "        lengths = torch.tensor([elem.shape[0] for elem in mel_features], dtype=torch.int32)\n",
    "        return feats, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f24e31-b942-4271-b9cc-27712bdd82a9",
   "metadata": {},
   "source": [
    "# Define the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c806203-2db8-4739-9626-ff5ddfa6763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from lightning import ConformerRNNTModule\n",
    "sp_model_path = '/home/wonkyum/fc-asr/spm_unigram_1023.model'\n",
    "sp_model = spm.SentencePieceProcessor(model_file=sp_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b603e-cc62-4be6-bfe5-c54d4ef87cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnt_checkpoint_path = '/home/wonkyum/fc-asr/exp/checkpoints/epoch=19-step=1170045.ckpt'\n",
    "rnnt_module = ConformerRNNTModule.load_from_checkpoint(rnnt_checkpoint_path, sp_model=sp_model).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a78b892-78f5-44bb-aac2-0d445835956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnt_module.model.transcriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0bbe2-a707-4b7c-8573-ef6e779828aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerWithAveragePooling(torch.nn.Module):\n",
    "    def __init__(self, conformer_model, encoder_dim, num_classes):\n",
    "        super(ConformerWithAveragePooling, self).__init__()\n",
    "        self.conformer_model = conformer_model\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.classifier = torch.nn.Linear(encoder_dim, num_classes)\n",
    "        \n",
    "    def forward(self, input_features: torch.Tensor, input_lengths: torch.Tensor):\n",
    "        # input_features: [batch_size, seq_len, feature_dim]\n",
    "        \n",
    "        # Get Conformer encoder outputs\n",
    "        encoder_outputs = self.conformer_model(input_features, input_lengths)  # [batch_size, seq_len, encoder_dim]\n",
    "        \n",
    "        # Apply average pooling\n",
    "        # Assuming encoder_outputs is a tensor; if it's a tuple, adjust accordingly\n",
    "        pooled_output = torch.mean(encoder_outputs[0], dim=1)  # [batch_size, encoder_dim]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)  # [batch_size, num_classes]\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962dfc6-6c68-45ca-bb60-82a03628be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConformerWithAveragePooling(conformer_model=rnnt_module.model.transcriber, encoder_dim=1024, num_classes=2)\n",
    "model.to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e8cf5-35b4-4ca9-8770-8ea30981bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409d412-2b82-4670-b592-185ef0898e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.conformer_model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6454ace-c7b9-4922-9b62-071a01a81a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3123172a-65c6-4f84-bd62-f64aa9cc240a",
   "metadata": {},
   "source": [
    "# Training and Testing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd970e92-2ee8-4d62-843f-2515241c8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    celoss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        feats, lengths = transform(data)\n",
    "\n",
    "        output = model(feats.to(device), lengths.to(device))\n",
    "        \n",
    "        loss = celoss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ad63a-035e-4ab6-8b90-80e6a8ff5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def test(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        feats, lengths = transform(data)\n",
    "\n",
    "        feats.to(device)\n",
    "        lengths.to(device)\n",
    "\n",
    "        output = model(feats.to(device), lengths.to(device))\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734dc88f-fba1-4eff-8238-56dcf6d3807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 20\n",
    "n_epoch = 20\n",
    "\n",
    "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "losses = []\n",
    "\n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "transform = TransfomrFeature('./global_stats.json')\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        train(model, epoch, log_interval)\n",
    "        test(model, epoch)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f09102-6189-43cf-810e-7c9fae709570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    # Use the model to predict the label of the waveform\n",
    "    feats, lengths = transform(tensor)\n",
    "    output = model(feats.to(device), lengths.to(device))\n",
    "    output = get_likely_index(output)\n",
    "    output = index_to_label(output.squeeze(0))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6f957-a430-4121-9159-fd34bb714bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate, label, *_ = train_set[5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1124a2-b24c-4ee2-b1d7-7eceb41341cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(waveform.numpy(), rate=sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d448caf-5942-4a0b-8b3d-e17a2a79d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(waveform), label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32bd29a-2a51-4aff-8f56-ebcf1b4fb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate, label, *_ = test_set[500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abed63b-6ae3-4ee2-a4e0-7f18445b3d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(waveform.numpy(), rate=sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca634c-f50f-4a0a-ba2d-f3f549f902d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(waveform), label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f27ded-5ede-48ca-bfc8-f80a660d2595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
