from typing import List
import random
import io
import base64
import time
import pandas as pd
import streamlit as st
from audio_recorder_streamlit import audio_recorder
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain.schema import StrOutputParser, AIMessage, HumanMessage, SystemMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

# Set page configuration for wide layout
st.set_page_config(layout="wide")

if "curr_page" not in st.session_state:
    st.session_state["curr_page"] = "home"
    st.session_state["curr_topic"] = "home"

if "prev_audio_bytes" not in st.session_state:
    st.session_state.prev_audio_bytes = None

if "exam_context" not in st.session_state:
    st.session_state.exam_context = {}


client = OpenAI()


def autoplay_audio(file_path: str):
    with open(file_path, "rb") as f:
        data = f.read()
        b64 = base64.b64encode(data).decode()
        md = f"""
            <audio controls autoplay="true">
            <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
            </audio>
            """
        st.markdown(
            md,
            unsafe_allow_html=True,
        )


def recognize_speech():
    user_input = ""
    # ì§ˆë¬¸ì— ë‹µí•˜ê¸°
    audio_bytes = audio_recorder(
        "talk",
        pause_threshold=3.0,
    )
    if audio_bytes == st.session_state.prev_audio_bytes:
        audio_bytes = None
    st.session_state.prev_audio_bytes = audio_bytes

    try:
        if audio_bytes:
            with st.spinner("ìŒì„± ì¸ì‹ì¤‘..."):
                with open("./tmp_audio.wav", "wb") as f:
                    f.write(audio_bytes)

                with open("./tmp_audio.wav", "rb") as f:
                    transcript = client.audio.transcriptions.create(
                        model="whisper-1", file=f, language="en"
                    )
                    user_input = transcript.text
    except Exception as e:
        print(e)
        pass
    return user_input


# Assuming you have a dictionary that holds your data like below:
speaking_topic_to_topic_info_map = {
    "speaking__listen_and_answer": {
        "display_name": "ë“£ê³  ì§ˆë¬¸ì— ë‹µí•˜ê¸°",
        "emoji": "ğŸ’­",
    },
    "speaking__express_an_opinion": {"display_name": "ì˜ê²¬ ë§í•˜ê¸°", "emoji": "ğŸ—£ï¸"},
    "speaking__debate": {"display_name": "í† ë¡ í•˜ê¸°", "emoji": "ğŸ‘©â€"},
    "speaking__describe_img": {"display_name": "ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸°", "emoji": "ğŸï¸"},
    "speaking__describe_charts": {"display_name": "ë„í‘œ ë³´ê³  ë°œí‘œí•˜ê¸°", "emoji": "ğŸ“Š"},
}

writing_topic_to_topic_info_map = {
    "writing__dictation": {"display_name": "ë°›ì•„ì“°ê¸° ì‹œí—˜ ìœ í˜• ë§Œë“¤ê¸°", "emoji": "âœï¸"},
    "writing__responding_to_an_email": {
        "display_name": "ì´ë©”ì¼ ë‹µì¥í•˜ê¸°",
        "emoji": "âœ‰ï¸",
    },
    "writing__summarization": {"display_name": "ì œì‹œë¬¸ ë‚´ìš©ì„ ìš”ì•½í•˜ê¸°", "emoji": "âœï¸"},
    "writing__writing_opinion": {"display_name": "ìì‹ ì˜ ì˜ê²¬ì“°ê¸°", "emoji": "ğŸ“"},
}


def go_to_topic(topic):
    st.session_state["curr_page"] = topic
    st.session_state["curr_topic"] = topic


def go_to_result():
    st.session_state["curr_page"] = "result"


# Create a function to display each topic in the grid
def display_topic(topic, topic_info, key):
    with st.container(border=True):
        st.write(f"{topic_info['emoji']} **{topic_info['display_name']}**")
        st.button(
            "ì‹œì‘",
            key=f"start_{topic}_{key}",
            on_click=go_to_topic,
            kwargs=dict(topic=topic),
        )


con = st.container()
if st.session_state["curr_page"] == "home":
    with con:
        st.title("Speaking & Writing ì–´í•™ ì‹œí—˜")

        tab1, tab2 = st.tabs(["Speaking ì‹œí—˜", "Writing ì‹œí—˜"])

        with tab1:

            cols = st.columns(2)
            for i, (topic, topic_info) in enumerate(
                speaking_topic_to_topic_info_map.items()
            ):
                with cols[i % 2]:  # This will alternate between the two columns
                    display_topic(topic, topic_info, i)

        with tab2:
            cols = st.columns(2)
            for i, (topic, topic_info) in enumerate(
                writing_topic_to_topic_info_map.items()
            ):
                with cols[i % 2]:  # This will alternate between the two columns
                    display_topic(topic, topic_info, i)


elif st.session_state["curr_page"] == "speaking__listen_and_answer":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info["display_name"])

    # random í•˜ê²Œ ì§ˆë¬¸ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    @st.cache_data
    def load_listen_and_answer_data():
        df = pd.read_csv("./data/speaking__listen_and_answer/question_and_audio.csv")
        return df

    df = load_listen_and_answer_data()

    if "question" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]

        question = sample["question"]
        audio_file_path = sample["audio_file_path"]

        st.session_state.exam_context["sample"] = sample
        st.session_state.exam_context["question"] = question
        st.session_state.exam_context["audio_file_path"] = audio_file_path

    if st.button("ì‹œí—˜ ì‹œì‘"):
        st.session_state.exam_context["exam_start"] = True
        st.session_state.exam_context["do_speech"] = True

    if st.session_state.exam_context.get("exam_start", False):
        if st.session_state.exam_context["do_speech"]:
            autoplay_audio(st.session_state.exam_context["audio_file_path"])
            st.session_state.exam_context["do_speech"] = False

        if not st.session_state.exam_context["do_speech"]:
            recognized_text = recognize_speech()
            st.session_state.exam_context["user_answer"] = recognized_text

        if st.session_state.exam_context.get("user_answer"):

            with st.container(border=True):
                answer_text = f"""
                - Question: {st.session_state.exam_context["question"]}
                - Your Answer: {st.session_state.exam_context.get("user_answer")}
                """

                st.markdown(answer_text)

            def get_speaking__listen_and_answer_result(answer_text):
                model = ChatOpenAI(model="gpt-4-1106-preview")

                class Score(BaseModel):
                    reason: str = Field(
                        description="Questionì— ëŒ€í•´ Your Answerê°€ ì ì ˆí•œì§€ì— ëŒ€í•´ ì¶”ë¡ í•˜ë¼. í•œêµ­ì–´ë¡œ."
                    )
                    score: int = Field(
                        description="Questionì— ëŒ€í•´ Your Answerê°€ ì ì ˆí•œì§€ì— ëŒ€í•´ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼"
                    )

                parser = JsonOutputParser(pydantic_object=Score)
                format_instruction = parser.get_format_instructions()

                human_msg_prompt_template = HumanMessagePromptTemplate.from_template(
                    "{input}\n---\nQuestionì— ëŒ€í•´ Your Answerê°€ ì ì ˆí•œì§€ì— ëŒ€í•´ ì¶”ë¡ í•´ì„œ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ë¼. ë‹¤ìŒì˜ í¬ë§·ì— ë§ì¶° ì‘ë‹µí•´ë¼.  : {format_instruction}",
                    partial_variables={"format_instruction": format_instruction},
                )

                prompt_template = ChatPromptTemplate.from_messages(
                    [human_msg_prompt_template],
                )

                chain = prompt_template | model | parser
                return chain.invoke({"input": answer_text})

            with st.container(border=True):
                """
                ### í‰ê°€ ê²°ê³¼
                """

                with st.spinner("ì±„ì ì¤‘..."):
                    result = get_speaking__listen_and_answer_result(answer_text)

                f"""
                {result['reason']}

                #### ì´ì : {result['score']}

                """

####################################
#
elif st.session_state["curr_page"] == "speaking__express_an_opinion":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info["display_name"])

    # random í•˜ê²Œ ì§ˆë¬¸ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    @st.cache_data
    def load_speaking__express_an_opinion_data():
        df = pd.read_csv("./data/speaking__express_an_opinion/question_and_audio.csv")
        return df

    df = load_speaking__express_an_opinion_data()

    if "question" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]

        question = sample["question"]
        audio_file_path = sample["audio_file_path"]

        st.session_state.exam_context["sample"] = sample
        st.session_state.exam_context["question"] = question
        st.session_state.exam_context["audio_file_path"] = audio_file_path

    if st.button("ì‹œí—˜ ì‹œì‘"):
        st.session_state.exam_context["exam_start"] = True
        st.session_state.exam_context["do_speech"] = True

    if st.session_state.exam_context.get("exam_start", False):
        if st.session_state.exam_context["do_speech"]:
            autoplay_audio(st.session_state.exam_context["audio_file_path"])
            st.session_state.exam_context["do_speech"] = False

        if not st.session_state.exam_context["do_speech"]:
            recognized_text = recognize_speech()
            st.session_state.exam_context["user_answer"] = recognized_text

        if st.session_state.exam_context.get("user_answer"):

            with st.container(border=True):
                answer_text = f"""
                - Question: {st.session_state.exam_context["question"]}
                - Your Answer: {st.session_state.exam_context.get("user_answer")}
                """

                st.markdown(answer_text)

            with st.container(border=True):

                def get_speaking__express_opinion_result(answer_text):
                    model = ChatOpenAI(model="gpt-4-1106-preview")

                    class Score(BaseModel):
                        reason: str = Field(
                            description="Questionì— ëŒ€í•´ ì˜ê²¬ì„ ë§í•˜ëŠ” ì‹œí—˜ì´ë‹¤. ì˜ê²¬ì„ ì ì ˆíˆ êµ¬ì¡°ì ìœ¼ë¡œ ì‘ë‹µí–ˆëŠ”ì§€ ì¶”ë¡ í•˜ë¼. í•œêµ­ì–´ë¡œ."
                        )
                        score: int = Field(
                            description="Questionì— ëŒ€í•´ Your Answerê°€ ì¶©ë¶„íˆ ë…¼ë¦¬ì ìœ¼ë¡œ ì˜ê²¬ì„ í‘œí˜„í–ˆëŠ”ì§€ì— ëŒ€í•´ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼."
                        )

                    parser = JsonOutputParser(pydantic_object=Score)
                    format_instruction = parser.get_format_instructions()

                    human_msg_prompt_template = HumanMessagePromptTemplate.from_template(
                        "{input}\n---\nQuestionì— ëŒ€í•´ Your Answerê°€ ì¶©ë¶„íˆ ë…¼ë¦¬ì ìœ¼ë¡œ ì˜ê²¬ì„ í‘œí˜„í–ˆëŠ”ì§€ì— ëŒ€í•´ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼. ë‹¤ìŒì˜ í¬ë§·ì— ë§ì¶° ì‘ë‹µí•´ë¼.  : {format_instruction}",
                        partial_variables={"format_instruction": format_instruction},
                    )

                    prompt_template = ChatPromptTemplate.from_messages(
                        [human_msg_prompt_template],
                    )

                    chain = prompt_template | model | parser
                    return chain.invoke({"input": answer_text})

                """
                ### í‰ê°€ ê²°ê³¼
                """

                with st.spinner("ì±„ì ì¤‘..."):
                    result = get_speaking__express_opinion_result(answer_text)

                f"""
                {result['reason']}

                #### ì´ì : {result['score']}

                """


elif st.session_state["curr_page"] == "speaking__debate":

    st.title("í† ë¡ í•˜ê¸°")

    con1 = st.container()
    con2 = st.container()

    user_input = ""

    if "model" not in st.session_state.exam_context:
        st.session_state.exam_context["model"] = ChatOpenAI(model="gpt-3.5-turbo")

    if "messages" not in st.session_state.exam_context:
        system_prompt = """\
- ë„ˆëŠ” AI ì‹œí—˜ ê°ë…ì´ë‹¤.
- userì˜ ì˜ì–´ ì‹¤ë ¥ì„ ìœ„í•´ ì–´ë– í•œ ì£¼ì œì— ëŒ€í•´ ì„œë¡œ ì§ˆë¬¸ê³¼ ë‹µì„í•˜ë©° í† ë¡ í•œë‹¤."""

        model = st.session_state.exam_context["model"]
        question = model.invoke("Create a controversial question for me.").content

        st.session_state.exam_context["messages"] = [
            SystemMessage(content=system_prompt),
            AIMessage(content=question),
        ]

        speech_file_path = "tmp_speak.mp3"
        response = client.audio.speech.create(
            model="tts-1",
            voice="alloy",  # alloy, echo, fable, onyx, nova, and shimmer
            input=question,
        )
        response.stream_to_file(speech_file_path)
        autoplay_audio(speech_file_path)

    with con1:
        for message in st.session_state.exam_context["messages"]:
            if isinstance(message, SystemMessage):
                continue
            role = "user" if message.type == "human" else "assistant"
            with st.chat_message(role):
                st.markdown(message.content)

    with con2:
        user_input = recognize_speech()

    with con1:

        turn_len = len(st.session_state.exam_context["messages"])
        max_turn_len = 5

        if user_input and turn_len < max_turn_len:
            st.session_state.exam_context["messages"].append(
                HumanMessage(content=user_input)
            )

            with st.chat_message("user"):
                st.markdown(user_input)

            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""

                model = st.session_state.exam_context["model"]

                for chunk in model.stream(st.session_state.exam_context["messages"]):
                    full_response += chunk.content or ""
                    message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)

                speech_file_path = "tmp_speak.mp3"
                response = client.audio.speech.create(
                    model="tts-1",
                    voice="alloy",  # alloy, echo, fable, onyx, nova, and shimmer
                    input=full_response,
                )
                response.stream_to_file(speech_file_path)

                autoplay_audio(speech_file_path)

            st.session_state.exam_context["messages"].append(
                AIMessage(content=full_response)
            )

        if turn_len >= max_turn_len:

            def get_speaking__debate_result(conversation):
                model = ChatOpenAI(model="gpt-4-1106-preview")

                class Score(BaseModel):
                    reason: str = Field(
                        description="ì£¼ì–´ì§„ ëŒ€í™”ì— ëŒ€í•´ Userê°€ ì–¼ë§ˆë‚˜ ë…¼ë¦¬ì ì´ê³  ìœ ì°½í•˜ê²Œ ì˜ì–´ë¡œ ì‘ë‹µí•˜ì˜€ëŠ”ì§€ ì¶”ë¡ í•˜ë¼. í•œêµ­ì–´ë¡œ."
                    )
                    score: int = Field(
                        description="ì£¼ì–´ì§„ ëŒ€í™”ì—ì„œ Userì˜ ì‘ë‹µì— ëŒ€í•´ ìœ ì°½ì„±ê³¼ ë…¼ë¦¬ì„±ì„ ê³ ë ¤í•˜ì—¬ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼."
                    )

                parser = JsonOutputParser(pydantic_object=Score)
                format_instruction = parser.get_format_instructions()

                human_msg_prompt_template = HumanMessagePromptTemplate.from_template(
                    "{input}\n---\nì£¼ì–´ì§„ ëŒ€í™”ì—ì„œ Userì˜ ì‘ë‹µì— ëŒ€í•´ ìœ ì°½ì„±ê³¼ ë…¼ë¦¬ì„±ì„ ê³ ë ¤í•˜ì—¬ 0~10ì  ì‚¬ì´ì˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ë¼. ë‹¤ìŒì˜ í¬ë§·ì— ë§ì¶° ì‘ë‹µí•´ë¼.  : {format_instruction}",
                    partial_variables={"format_instruction": format_instruction},
                )

                prompt_template = ChatPromptTemplate.from_messages(
                    [human_msg_prompt_template],
                )

                chain = prompt_template | model | parser
                return chain.invoke({"input": conversation})

            with st.container(border=True):
                """
                ### í‰ê°€ ê²°ê³¼
                """

                with st.spinner("ì±„ì ì¤‘..."):

                    conversation = ""
                    for msg in st.session_state.exam_context["messages"]:
                        role = "User" if msg.type == "human" else "AI"
                        conversation += f"{role}: {msg.content}"

                    result = get_speaking__debate_result(conversation)

                grade = ""

                if result["score"] >= 8:
                    grade = "Advanced"
                elif 4 < result["score"] < 8:
                    grade = "Intermediate"
                elif result["score"] <= 4:
                    grade = "Novice"

                grade = f"{grade}, {result['score']}"

                f"""
                {result['reason']}

                #### ë“±ê¸‰: {grade}
                """


elif st.session_state["curr_page"] == "speaking__describe_img":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info["display_name"])

    # random í•˜ê²Œ ì§ˆë¬¸ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    @st.cache_data
    def load_speaking__describe_img():
        df = pd.read_csv("./data/speaking__describe_img/desc_img.csv")
        return df

    df = load_speaking__describe_img()

    if "img_path" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]

        img_path = sample["img_path"]
        desc = sample["desc"]

        st.session_state.exam_context["img_path"] = img_path
        st.session_state.exam_context["desc"] = desc
        st.session_state.exam_context["recognized_text"] = ""

    st.image(st.session_state.exam_context["img_path"])

    with st.container(border=True):
        recognized_text = recognize_speech()
        if recognized_text:
            st.session_state.exam_context["recognized_text"] = recognized_text
        st.write(st.session_state.exam_context["recognized_text"])

    submit = st.button("ì œì¶œí•˜ê¸°")

    if submit:

        def get_speaking__describe_img(user_input, ref):
            model = ChatOpenAI(
                model="gpt-4-1106-preview", temperature=0.8
            )  # CoT ëŠ” ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ë§Œë“¤ì–´ì•¼í•˜ê¸° ë•Œë¬¸ì— temperatureë¥¼ ì˜¬ë ¤ì•¼í•¨

            class Evaluation(BaseModel):
                score: int = Field(description="ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸° í‘œí˜„ í‘œí˜„ ì ìˆ˜. 0~10ì ")
                feedback: str = Field(
                    description="ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸°ë¥¼ ë” ì˜ í•  ìˆ˜ ìˆë„ë¡í•˜ëŠ” ìì„¸í•œ í”¼ë“œë°±. Markdowní˜•ì‹, í•œêµ­ì–´ë¡œ."
                )

            parser = JsonOutputParser(pydantic_object=Evaluation)
            format_instructions = parser.get_format_instructions()

            human_prompt_template = HumanMessagePromptTemplate.from_template(
                "ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸° ì˜ì–´ ì‹œí—˜ì´ë‹¤. ì‚¬ìš©ìì˜ ì‘ë‹µì„ Referenceì™€ ë¹„êµí•˜ì—¬ í‰ê°€í•˜ë¼.\nì‚¬ìš©ì: {input}\Reference: {ref}\n{format_instructions}",
                partial_variables={"format_instructions": format_instructions},
            )

            prompt = ChatPromptTemplate.from_messages(
                [
                    human_prompt_template,
                ]
            )
            eval_chain = prompt | model | parser

            result = eval_chain.invoke({"input": user_input, "ref": ref})
            return result

        st.title("ê²°ê³¼ & í”¼ë“œë°±- ì‚¬ì§„ ë¬˜ì‚¬í•˜ê¸°")

        with st.spinner("ê²°ê³¼ & í”¼ë“œë°± ìƒì„±ì¤‘..."):

            result = get_speaking__describe_img(
                user_input=recognized_text, ref=st.session_state.exam_context["desc"]
            )

            grade = ""
            if result["score"] >= 8:
                grade = "ê³ ê¸‰"
            elif 4 < result["score"] < 8:
                grade = "ì¤‘ê¸‰"
            elif result["score"] <= 4:
                grade = "ì´ˆê¸‰"

            grade = f"{grade} ({result['score']}/10)"

            f"""
            ë‹¹ì‹ ì´ ì œê³µí•œ ë‹µë³€ì€ ìŠ¤í”¼í‚¹ ì‚¬ì§„ ë¬˜ì‚¬ ì‹œí—˜ì—ì„œ `{grade}` ìˆ˜ì¤€ìœ¼ë¡œ ì‹œì‘í•˜ê¸° ì¢‹ì€ ì ‘ê·¼ì…ë‹ˆë‹¤.
            
            ì—¬ê¸° ëª‡ê°€ì§€ í”¼ë“œë°±ì„ ë“œë¦½ë‹ˆë‹¤.

            {result['feedback']}
            """


elif st.session_state["curr_page"] == "speaking__describe_charts":
    topic_info = speaking_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info["display_name"])

    # random í•˜ê²Œ ì§ˆë¬¸ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    @st.cache_data
    def load_speaking__describe_charts():
        df = pd.read_csv("./data/speaking__describe_charts/desc_charts.csv")
        return df

    df = load_speaking__describe_charts()

    if "img_path" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]

        img_path = sample["img_path"]
        desc = sample["desc"]

        st.session_state.exam_context["img_path"] = img_path
        st.session_state.exam_context["desc"] = desc
        st.session_state.exam_context["recognized_text"] = ""

    st.image(st.session_state.exam_context["img_path"])

    with st.container(border=True):
        recognized_text = recognize_speech()
        if recognized_text:
            st.session_state.exam_context["recognized_text"] = recognized_text
        st.write(st.session_state.exam_context["recognized_text"])

    submit = st.button("ì œì¶œí•˜ê¸°")

    if submit:

        def get_speaking__describe_img(user_input, ref):
            model = ChatOpenAI(
                model="gpt-4-1106-preview", temperature=0.8
            )  # CoT ëŠ” ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ë§Œë“¤ì–´ì•¼í•˜ê¸° ë•Œë¬¸ì— temperatureë¥¼ ì˜¬ë ¤ì•¼í•¨

            class Evaluation(BaseModel):
                score: int = Field(description="ë„í‘œ ë³´ê³  ë°œí‘œí•˜ê¸° ì ìˆ˜. 0~10ì ")
                feedback: str = Field(
                    description="ë„í‘œ ë³´ê³  ë°œí‘œí•˜ê¸° ì ìˆ˜. Markdowní˜•ì‹, í•œêµ­ì–´ë¡œ."
                )

            parser = JsonOutputParser(pydantic_object=Evaluation)
            format_instructions = parser.get_format_instructions()

            human_prompt_template = HumanMessagePromptTemplate.from_template(
                "ë„í‘œë³´ê³  ë°œí‘œí•˜ê¸° ì˜ì–´ ì‹œí—˜ì´ë‹¤. ì‚¬ìš©ìì˜ ì‘ë‹µì„ Referenceì™€ ë¹„êµí•˜ì—¬ í‰ê°€í•˜ë¼.\nì‚¬ìš©ì: {input}\Reference: {ref}\n{format_instructions}",
                partial_variables={"format_instructions": format_instructions},
            )

            prompt = ChatPromptTemplate.from_messages(
                [
                    human_prompt_template,
                ]
            )
            eval_chain = prompt | model | parser

            result = eval_chain.invoke({"input": user_input, "ref": ref})
            return result

        st.title("ê²°ê³¼ & í”¼ë“œë°±- ë„í‘œ ë³´ê³  ë°œí‘œí•˜ê¸°")

        with st.spinner("ê²°ê³¼ & í”¼ë“œë°± ìƒì„±ì¤‘..."):

            result = get_speaking__describe_img(
                user_input=recognized_text, ref=st.session_state.exam_context["desc"]
            )

            grade = ""
            if result["score"] >= 8:
                grade = "ê³ ê¸‰"
            elif 4 < result["score"] < 8:
                grade = "ì¤‘ê¸‰"
            elif result["score"] <= 4:
                grade = "ì´ˆê¸‰"

            grade = f"{grade} ({result['score']}/10)"

            f"""
            ë‹¹ì‹ ì´ ì œê³µí•œ ë‹µë³€ì€ ìŠ¤í”¼í‚¹ ì‚¬ì§„ ë¬˜ì‚¬ ì‹œí—˜ì—ì„œ `{grade}` ìˆ˜ì¤€ìœ¼ë¡œ ì‹œì‘í•˜ê¸° ì¢‹ì€ ì ‘ê·¼ì…ë‹ˆë‹¤.
            
            ì—¬ê¸° ëª‡ê°€ì§€ í”¼ë“œë°±ì„ ë“œë¦½ë‹ˆë‹¤.

            {result['feedback']}
            """


elif st.session_state["curr_page"] == "writing__dictation":
    from utils import grade_dictation

    topic_info = writing_topic_to_topic_info_map[st.session_state.curr_topic]
    st.title(topic_info["display_name"])

    # random í•˜ê²Œ ì§ˆë¬¸ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    @st.cache_data
    def load_writing__dictation():
        df = pd.read_csv("./data/writing__dictation/sent_and_audio.csv")
        return df

    df = load_writing__dictation()

    if "sentence" not in st.session_state.exam_context:
        sample = df.sample(n=1).iloc[0]

        sentence = sample["sentence"]
        audio_file_path = sample["audio_file_path"]

        st.session_state.exam_context["sample"] = sample
        st.session_state.exam_context["sentence"] = sentence
        st.session_state.exam_context["audio_file_path"] = audio_file_path

    if st.button("ì‹œí—˜ ì‹œì‘"):
        st.session_state.exam_context["exam_start"] = True
        st.session_state.exam_context["do_speech"] = True

    if st.session_state.exam_context.get("exam_start", False):
        if st.session_state.exam_context["do_speech"]:
            autoplay_audio(st.session_state.exam_context["audio_file_path"])
            st.session_state.exam_context["do_speech"] = False

        user_answer = st.text_input("user answer")
        if user_answer:
            st.session_state.exam_context["user_answer"] = user_answer

        if st.session_state.exam_context.get("user_answer"):

            with st.container(border=True):
                answer_text = f"""
                - Original sentence: {st.session_state.exam_context["sentence"]}
                - Your Answer: {st.session_state.exam_context.get("user_answer")}
                """

                st.markdown(answer_text)

            def get_writing__dictation_result(answer_text, ref):
                model = ChatOpenAI(model="gpt-4-1106-preview")

                class Evaluation(BaseModel):
                    reason: str = Field(description="ë°›ì•„ì“°ê¸° í‰ê°€ë¥¼ ìœ„í•œ ì¶”ë¡ ")
                    score: int = Field(description="ë°›ì•„ì“°ê¸° ì ìˆ˜. 0~10ì ")

                parser = JsonOutputParser(pydantic_object=Evaluation)
                format_instruction = parser.get_format_instructions()

                human_prompt_template = HumanMessagePromptTemplate.from_template(
                    "ì˜ì–´ ë°›ì•„ì“°ê¸° ì‹œí—˜ì´ë‹¤. ì‚¬ìš©ìì˜ ì‘ë‹µì„ Referenceì™€ ë¹„êµí•˜ì—¬ í‰ê°€í•˜ë¼.\nì‚¬ìš©ì: {input}\Reference: {ref}\n{format_instructions}",
                    partial_variables={"format_instructions": format_instruction},
                )

                prompt_template = ChatPromptTemplate.from_messages(
                    [
                        human_prompt_template,
                    ]
                )

                chain = prompt_template | model | parser
                return chain.invoke({"input": answer_text, "ref": ref})

            with st.container(border=True):
                """
                ### í‰ê°€ ê²°ê³¼
                """

                with st.spinner("ì±„ì ì¤‘..."):
                    model_result = get_writing__dictation_result(
                        answer_text, st.session_state.exam_context["sentence"]
                    )
                    automatic_result = grade_dictation(
                        correct_script=st.session_state.exam_context["sentence"],
                        student_response=answer_text,
                    )

                    model_score = model_result["score"]
                    automatic_score = automatic_result["accuracy"] * 10
                    final_score = (model_score + automatic_score) / 2

                f"""
                ### Model í‰ê°€ ê²°ê³¼
                {model_result['reason']}
                ì ìˆ˜: {model_score}

                ### Automatic í‰ê°€ ê²°ê³¼
                ì ìˆ˜: {automatic_score}

                #### ì´ì : {final_score}

                """
