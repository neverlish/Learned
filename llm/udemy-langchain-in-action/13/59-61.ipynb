{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ab8468-8c44-49a1-8d72-cd6e23dc270a",
   "metadata": {},
   "source": [
    "# LangChain Expression Language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8043f886-3b44-414f-8f24-919173a7290a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c209b-789b-4603-a1d9-de213d5a4b21",
   "metadata": {},
   "source": [
    "Old way\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6463875d-ab54-4591-be52-be8086ea17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b197f411-c77b-4b63-bea6-7c0547f5f9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyeonjinho/.pyenv/versions/3.9.18/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39f1741-a982-4483-90ed-3771ae5ce2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me about the nutritional value of {input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1746e3ea-d9c0-4d02-94e4-1e786bbe6ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pizza can vary greatly in nutritional value depending on the toppings and crust used. However, a typical slice of cheese pizza (1/8 of a 14-inch pizza) has approximately 285 calories, 10 grams of fat, 36 grams of carbohydrates, and 12 grams of protein.\\n\\nPizza can be a good source of several essential nutrients, including calcium from the cheese, protein from the cheese and crust, and vitamins and minerals from the tomato sauce and toppings. However, it can also be high in sodium, saturated fat, and calories, especially if it is loaded with high-fat meats and extra cheese.\\n\\nTo make pizza more nutritious, consider opting for a whole wheat crust, adding plenty of vegetables as toppings, and choosing lean proteins like grilled chicken or shrimp. Additionally, limiting portion sizes and enjoying pizza in moderation can help prevent it from becoming a high-calorie, high-fat meal.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.predict(input=\"Pizza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b48b28b-f0fc-4035-9131-af332fdec67d",
   "metadata": {},
   "source": [
    "New way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c27aab82-36f1-4e73-b1ef-9bd2a6300a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Spaghetti is a popular pasta dish made from durum wheat semolina flour and water. It is a good source of carbohydrates, providing energy to fuel the body. A one-cup serving of cooked spaghetti contains approximately 200 calories, 40 grams of carbohydrates, 7 grams of protein, and less than 1 gram of fat.\\n\\nSpaghetti also contains essential vitamins and minerals such as iron, magnesium, and B vitamins. Iron is important for the production of red blood cells and magnesium is essential for muscle and nerve function. B vitamins play a role in energy metabolism and overall health.\\n\\nSpaghetti is typically served with a tomato-based sauce, which adds additional nutrients such as vitamin C and lycopene. Lycopene is a powerful antioxidant that may help reduce the risk of certain chronic diseases.\\n\\nOverall, spaghetti can be a nutritious and satisfying meal when paired with a balanced sauce and other healthy ingredients such as vegetables and lean protein. It is important to watch portion sizes and choose whole grain spaghetti for added fiber and nutrients.', response_metadata={'token_usage': {'completion_tokens': 205, 'prompt_tokens': 16, 'total_tokens': 221}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-9e76cbc6-89db-40ca-bb4a-f7d516ed55f1-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain.invoke({\"input\": \"Spaghetti\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c25b25-8732-4111-9ec2-9f811ff1d9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'input': {'title': 'Input', 'type': 'string'}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b74099b-9580-4b84-bf7a-65d4bf3166e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'input': {'title': 'Input', 'type': 'string'}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b2bfce-f5a9-45e7-8c52-f5d254ff05e1",
   "metadata": {},
   "source": [
    "# First pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "488c802b-7c52-4d33-a973-9d49df8af4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a26fb08-7be5-481e-9587-fa85fbb72ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lasagna is a classic Italian dish made with layers of pasta, cheese, meat, and tomato sauce. It is a rich and satisfying meal that provides a good source of essential nutrients.\\n\\nThe nutritional value of lasagna can vary depending on the specific ingredients used in the recipe. However, a typical serving of lasagna (about 1 cup) can provide the following nutrients:\\n\\n- Protein: Lasagna is a good source of protein, thanks to the meat and cheese layers. Protein is important for building and repairing tissues in the body.\\n- Carbohydrates: The pasta in lasagna provides a source of carbohydrates, which are the body's main source of energy.\\n- Fat: Lasagna can be high in fat, especially if it contains a lot of cheese or meat. However, some fats, such as those found in olive oil, can be beneficial for heart health.\\n- Fiber: Depending on the type of pasta and vegetables used, lasagna can also be a good source of dietary fiber, which is important for digestive health.\\n- Vitamins and minerals: Lasagna can provide essential vitamins and minerals such as calcium, vitamin A, vitamin C, and iron. These nutrients are important for overall health and well-being.\\n\\nIt's important to keep in mind that lasagna can be high in calories, so it's best enjoyed in moderation as part of a balanced diet. Additionally, choosing lean meats, whole grain pasta, and plenty of vegetables can help boost the nutritional value of this delicious dish.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"input\": \"Lasagna\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78d075ce-88cd-4e1a-a2fc-164de8df1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me 5 jokes about {input}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7718c38-1173-4348-a87f-c3ffc017106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm.bind(stop=[\"\\n\"]) | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01508f2c-b6c2-40d9-9d66-3ebca6eff6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Why did the pizza maker go to therapy? Because he kneaded it.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"pizzas\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b425f740-1cb4-4a12-bd52-3ec78b3b4c11",
   "metadata": {},
   "source": [
    "# OPENAI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79e11903-c8c9-442f-887e-e65d3c79e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"joke\",\n",
    "      \"description\": \"A joke\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"setup\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The setup for the joke\"\n",
    "          },\n",
    "          \"punchline\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The punchline for the joke\"\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"setup\", \"punchline\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dce0740-168a-4db6-b575-209202600ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "chain = (\n",
    "    prompt\n",
    "    | llm.bind(function_call={\"name\": \"joke\"}, functions= functions)\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17a613bf-5630-43dc-9a6c-66b9d421c447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't bears like fast food?\",\n",
       " 'punchline': \"Because they can't catch it!\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={\"input\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0b0cb-3e1c-4102-9f93-e64e263ea777",
   "metadata": {},
   "source": [
    "# Working with vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab4b2112-4b1a-4d9d-b25b-561c27010db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_texts([\"Cats are typically 9.1 kg in weight.\",\n",
    "                                 \"Cats have retractable claws.\",\n",
    "                                 \"A group of cats is called a clowder.\",\n",
    "                                 \"Cats can rotate their ears 180 degrees.\",\n",
    "                                 \"The world's oldest cat lived to be 38 years old.\"],\n",
    "                                embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "377bf299-4e7e-4e8d-839e-23a6acb38ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d4e650b-1c13-4c03-a908-837f69767807",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeb63b41-5cc1-474f-a15c-f302dcba5092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The oldest cat lived to be 38 years old.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"how old is the oldest cat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df79e159-e5f0-4e9e-96bb-bfb4a0e92894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ee0de95-9601-44cf-a60f-277564d7ae8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "operator.itemgetter('bla')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = {\"bla\": \"test\", \"x\": \"hi\"}\n",
    "itemgetter(\"bla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "680e7dc1-2c78-44ca-9207-e1b1a6ff4143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bla = itemgetter(\"bla\")\n",
    "get_bla(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c8e9f13-9081-48ba-801b-cf4e9c99d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = {\n",
    "    \"context\": itemgetter(\"question\") | retriever,\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"language\": itemgetter(\"language\")\n",
    "} | prompt | llm | StrOutputParser()\n",
    "\n",
    "# chain = {\n",
    "#     \"context\": (lambda x: x[\"question\"]) | retriever,\n",
    "#     \"question\": (lambda x: x[\"question\"]),\n",
    "#     \"language\": (lambda x: x[\"language\"])\n",
    "# } | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff303b2a-e457-4c4b-8d5d-53b1a958f6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'38 Jahre alt.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"how old is the oldest cat?\", \"language\": \"german\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4750a136-52f3-445d-bf59-629a4edaec86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckduckgo-search\n",
      "  Downloading duckduckgo_search-5.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: click>=8.1.7 in /Users/hyeonjinho/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from duckduckgo-search) (8.1.7)\n",
      "Collecting curl-cffi>=0.6.2 (from duckduckgo-search)\n",
      "  Downloading curl_cffi-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: orjson>=3.10.0 in /Users/hyeonjinho/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from duckduckgo-search) (3.10.0)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /Users/hyeonjinho/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from curl-cffi>=0.6.2->duckduckgo-search) (1.16.0)\n",
      "Requirement already satisfied: certifi in /Users/hyeonjinho/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from curl-cffi>=0.6.2->duckduckgo-search) (2023.11.17)\n",
      "Requirement already satisfied: pycparser in /Users/hyeonjinho/.pyenv/versions/3.9.18/lib/python3.9/site-packages (from cffi>=1.12.0->curl-cffi>=0.6.2->duckduckgo-search) (2.21)\n",
      "Downloading duckduckgo_search-5.3.0-py3-none-any.whl (21 kB)\n",
      "Downloading curl_cffi-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: curl-cffi, duckduckgo-search\n",
      "Successfully installed curl-cffi-0.6.2 duckduckgo-search-5.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17808552-0f74-4cd9-b100-f37098787b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "search = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8732234-d78c-41eb-aecc-1a2d6165c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"turn the following user input into a search query for a search engine:\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57aa82ac-086c-4134-be51-c0414fabdfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser() | search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc4cd7c6-a9d8-4d2f-9ab2-e87c2cfe402d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No good DuckDuckGo Search Result was found'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"whats the name of the oldest cat?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d34ad3-6e09-458c-a2d7-53e057e435e6",
   "metadata": {},
   "source": [
    "# Arbitrary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b455071-84c6-4c31-84c5-60ca1a859a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "def length_function(text):\n",
    "    return len(text)\n",
    "\n",
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "def multiple_length_function(_dict):\n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "chain = {\n",
    "    \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),\n",
    "    \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")} | RunnableLambda(multiple_length_function)\n",
    "} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e2ed96f-2f82-4b8e-a9a3-5070973b9114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 + 9 = 12'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824fdf2e-0c79-4ee5-a9f1-217a99f2a5ba",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e5c29b9-1b75-4401-8c05-8745b4815c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5f8a096-f18a-4e9f-bfe3-4fbc67de1717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the bear dissolve in water?\n",
      "Because it was a polar bear!"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(s.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c299039-e746-4db0-9284-9e96358c8a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why did the bear wear a fur coat to the party? \\nBecause he didn't want to be underdressed!\", response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 13, 'total_tokens': 37}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-dacfc482-d161-4274-b446-74791ac536a8-0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d0552ab-7e71-49b5-b189-18e5530deb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Why did the bear break up with his girlfriend? \\n\\nBecause he couldn't bear the relationship any longer!\", response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 13, 'total_tokens': 34}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-ae715fc9-9c9d-43a3-9ac6-ab8b61a10190-0'),\n",
       " AIMessage(content='Why was the cat sitting on the computer? \\n\\nBecause it wanted to keep an eye on the mouse!', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 13, 'total_tokens': 34}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-fe0a16fe-8fb5-4272-acca-048586adc97e-0')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef32a04d-242a-4fbc-91d2-9dc0c6160954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't bears like fast food? Because they can't catch it!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Why did the bear bring a flashlight to the party? \\nBecause he heard it was going to be a \"beary\" good time!', response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 13, 'total_tokens': 41}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-eb35d526-95f3-4544-869b-716d59a455e4-0')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async for s in chain.astream({\"topic\": \"bears\"}):\n",
    "    print(s.content, end=\"\")\n",
    "await chain.ainvoke({\"topic\": \"bears\"})\n",
    "await chain.abatch([{\"topic\": \"bears\"}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34803a37-c9ff-45ef-815c-9a88820f4ac3",
   "metadata": {},
   "source": [
    "# Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c458c41-cac1-4f6d-9a5b-84ba18812297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "chain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "chain2 = ChatPromptTemplate.from_template(\"tell me a bad joke about {topic}\") | model\n",
    "combined = RunnableParallel(joke=chain1, poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3afd0f99-7e67-489b-a19f-dd9b480c4528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What do you call a bear with no teeth?\\n\\nA gummy bear!', response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-80f81aa3-b513-45e9-9945-55276ff2367e-0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1.invoke({\"topic\": \"bears\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7fea004-7c00-4564-8724-1c6a75d3f0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the bear dissolve in water?\\n\\nBecause it was polar!', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 14, 'total_tokens': 27}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-ebadb9c9-d1c3-4d15-aa13-ffc39a51f87b-0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.invoke({\"topic\": \"bears\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0280d3c6-d90d-40a7-8c54-e38aeade0ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': AIMessage(content='Why did the bear dissolve in water?\\n\\nBecause it was polar!', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-23a14d36-be01-4fee-9c26-d8d852bbdda2-0'),\n",
       " 'poem': AIMessage(content='Why did the bear break up with his girlfriend?\\nBecause she was unbearable!', response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 14, 'total_tokens': 29}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-295ab752-c0a3-43f8-bb2e-8c48f341ef7a-0')}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.invoke({\"topic\": \"bears\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9bdbbe-80d1-44a5-9166-1c1fedc239b7",
   "metadata": {},
   "source": [
    "# Fallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6c269b5-5189-44c0-bc47-0235ce4fdc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a funny comedian and provide funny jokes about specific topics\"),\n",
    "        (\"human\", \"Make a joke about {input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fallback_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You tell the user that you currently are not able to make jokes since you are too tired\"),\n",
    "        (\"human\", \"Make a joke about {input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "bad_llm = ChatOpenAI(model_name=\"gpt-fake\")\n",
    "bad_chain = chat_prompt | bad_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c919997-abbb-4240-a1aa-90b3db1f7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()\n",
    "good_chain = fallback_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0909c428-8f9c-423a-911d-4a53479d6805",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = bad_chain.with_fallbacks([good_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4b9a3ba-a405-43cc-a5f4-c8ff94ae7e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but I'm currently too tired to come up with a joke about a cow. Maybe next time!\", response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 34, 'total_tokens': 58}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2b5c233-7e55-41f1-8042-5954323aac7b-0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"cow\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd72a2-3eff-497c-bddd-9f1bf36c7e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
