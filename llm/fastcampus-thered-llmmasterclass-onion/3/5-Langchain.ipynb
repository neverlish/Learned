{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1x/st3vh8xs6715dcgqc1gk2hhh0000gn/T/ipykernel_74923/2498872554.py:3: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\" },\\n        { \"농담은 제가 잘 모르는 것 같아요.\" }\\n    };\\n\\n    // 사용자의 말에 대응하는 챗봇의 응답\\n    private static final String[][] chatbotResponse = {\\n        { \"안녕하세요.\" },\\n        { \"좋은 아침이네요!\" },\\n        { \"제가 무엇을 도와드릴까요?\" },\\n        { \"저는 챗봇입니다.\" },\\n        { \"물어보는 내용이 이해되지 않네요.\",\\n          \"무슨 말씀이신지 이해되지 않네요.\",\\n          \"죄송합니다. 다시 말씀해주세요.\" },\\n        { \"알겠습니다.\" },\\n        { \"그래서요?\", \"무엇에 대해 궁금하신가요?\" },\\n        { \"무슨 말씀이신지 잘 모르겠어요.\" },\\n        { \"그게', generation_info={'finish_reason': 'length', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 10, 'completion_tokens': 256, 'total_tokens': 266}, 'model_name': 'gpt-3.5-turbo-instruct'} run=[RunInfo(run_id=UUID('cab840ce-e7d7-4534-9a56-2972457a4d5d'))] type='LLMResult'\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "response = llm.generate([\"농담 하나 해줘\"])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='두 개의 빵이 이야기를 하고 있었다. 하나가 다른 하나에게 물었다. \"왜 그렇게 찌푸리고 있어?\" \\n\\n다른 빵이 대답했다. \"나 오늘 베이킹되면서 너무 스트레스 받았어... 모든 건 너무 열빨에 쳐 있었어!\"\\n\\n첫 번째 빵이 웃으며 말했다, \"그건 너무 오밀조밀한 이야기야!\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 154, 'prompt_tokens': 17, 'total_tokens': 171, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BNfWIsJga528NRVlGYruuIrbrG43x', 'finish_reason': 'stop', 'logprobs': None}, id='run-49b0cd96-11af-4aed-9b90-2b0decb64ee3-0', usage_metadata={'input_tokens': 17, 'output_tokens': 154, 'total_tokens': 171, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "llm.invoke(\"농담 하나 해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyeonjinho/.pyenv/versions/3.11.6/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France? Paris\n",
      "\n",
      "The capital of France is Paris. It is the largest city in the country and is known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral. Paris is also a major global center for art, fashion, gastronomy, and culture. It has been the capital of France since 508 AD and is located on the Seine River in the northern part of the country.\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "repo_id = 'google/flan-t5-xxl'\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "  task=\"text-generation\",\n",
    "  repo_id=repo_id,huggingfacehub_api_token='')\n",
    "\n",
    "print(llm.invoke(\"What is the capital of France?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What NFL team won the Super Bowl in the year Justin Bieber was born?',\n",
       " 'text': ' Justin Bieber was born on March 1, 1994. The Super Bowl is played in February of each year. In 1994, the Super Bowl was played on January 30, 1994. The winners of that Super Bowl were the Dallas Cowboys, who defeated the Buffalo Bills by a score of 30-13. Therefore, the Dallas Cowboys won the Super Bowl in the year Justin Bieber was born.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
