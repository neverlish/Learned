{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3227b924-9ba0-40db-a906-1375b12796e8",
   "metadata": {},
   "source": [
    "# LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69abc03-a880-414d-8373-3c139dd941dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26ce9cd-663d-4705-b55b-a7350bbb7fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyeonjinho/.pyenv/versions/3.11.6/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba81dd0-8ba0-4754-8775-5f3ec857dc65",
   "metadata": {},
   "source": [
    "## Single Scoring\n",
    "- Self-Rewarding Language Models\n",
    "    - https://arxiv.org/pdf/2401.10020.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aca3058-8cce-478b-9ad5-9819d82cce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# judge_llm = ChatGoogleGenerativeAI(model='gemini-pro', temperature=0)\n",
    "judge_llm = ChatOpenAI(model='gpt-4-1106-preview', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7be266b-66cb-4088-b66e-04499a8b3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleScoringCoT(BaseModel):\n",
    "    thought: str = Field(description=\"Step-by-Step Thought Process\")\n",
    "    score: int = Field(description=\"score, 1~5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb52915-9055-457d-8c8e-53eb258da447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"thought\": {\"title\": \"Thought\", \"description\": \"Step-by-Step Thought Process\", \"type\": \"string\"}, \"score\": {\"title\": \"Score\", \"description\": \"score, 1~5\", \"type\": \"integer\"}}, \"required\": [\"thought\", \"score\"]}\\n```'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = JsonOutputParser(pydantic_object=SingleScoringCoT)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab7fd01-0f61-430c-8669-4e899bb8f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt_template_text = \"\"\"\\\n",
    "사용자의 질문과 후보 응답에 대한 평가를 다음 기준을 사용하여 5점 척도로 제공해주세요:\n",
    "1: 응답이 불완전하거나, 모호하거나, 주제에서 벗어났거나, 논란의 여지가 있거나, 사용자가 요청한 것과 정확히 일치하지 않습니다. 예를 들어, 일부 내용이 누락되었거나, 번호 매긴 목록이 처음부터 시작하지 않거나, 서두가 사용자의 질문을 반복합니다. 또는 응답이 다른 사람의 관점에서 그들의 개인 경험(예: 블로그 포스트에서 가져옴)으로 작성되었거나 포럼에서의 답변처럼 보입니다. 또는 홍보 텍스트, 네비게이션 텍스트 또는 기타 관련 없는 정보를 포함합니다.\n",
    "2: 응답이 사용자의 요청 대부분을 다룹니다. 사용자의 질문에 직접적으로 답하지 않습니다. 예를 들어, 사용자의 질문에 대한 정확한 해결책 대신 고차원적인 방법론만을 제공합니다.\n",
    "3: 응답이 유용하지만 AI 어시스턴트가 작성한 것이 아닙니다. 사용자의 기본적인 요구 사항을 모두 다룹니다. 완전하고 독립적이지만, 다른 사람의 관점에서 작성된 단점이 있습니다. 예를 들어, 개인 경험이나 의견을 포함하거나, 댓글 섹션 언급이나 소셜 미디어 공유 등이 포함됩니다.\n",
    "4: AI 어시스턴트의 관점에서 명확하게 사용자의 지시를 다루는 응답입니다. 사용자의 질문이나 지시에 대해 누락되거나 관련 없는 정보 없이 완전하고 명확하며 포괄적인 응답을 제공합니다. 잘 조직되어 있고, 자체 완결적이며, 도움이 되는 어조로 작성되었습니다. 약간의 개선 여지가 있습니다. 예를 들어, 더 간결하고 집중적일 수 있습니다.\n",
    "5: AI 어시스턴트의 완벽한 답변입니다. 사용자의 질문이나 지시를 해결하기 위해 의도적으로 작성된 것처럼 보이는 명확한 초점을 가지고 있습니다. 고품질의 내용을 제공하며, 해당 분야에서 전문 지식을 보여주며, 매우 잘 작성되었고 논리적이며, 따라하기 쉽고, 매력적이며, 통찰력이 있습니다.\n",
    "\n",
    "사용자: {question}\n",
    "<response>{response}</response>\n",
    "먼저 간략한 이유를 설명한 후 작성하세요. AI 어시스턴트의 스타일로 답변하세요. 최종 점수를 도출하기 위해 기준에 따라 단계별로 생각해보세요.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "judge_prompt_template = PromptTemplate.from_template(judge_prompt_template_text,\n",
    "                                                     partial_variables={\"format_instructions\": format_instructions})\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26bc8401-fb34-46f5-a9da-1ad25eda3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_chain = judge_prompt_template | judge_llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9aab39f-bf21-438f-af5b-a111a5e1f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"123+123\"\n",
    "\n",
    "# response = \"글쎄요 잘 모르겠네요\"\n",
    "response = \"123+123의 정답은 246입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6702d9-1ff7-4b23-a248-7cc078857794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thought': \"The response directly answers the user's mathematical query without any unnecessary information or errors. It is concise, accurate, and directly to the point, which aligns with the user's request for a simple arithmetic solution. There is no additional context or explanation, but none is needed for such a straightforward question. The response does not include any personal opinions, external perspectives, or irrelevant content. It is written from the AI assistant's perspective, providing a clear and direct answer to the user's question. Therefore, it meets the criteria for a high score. However, it lacks an element of engagement or additional value that might be provided by explaining the calculation process or offering related educational content, which could elevate the response from being simply correct to being insightful or educational.\",\n",
       " 'score': 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge_chain.invoke({\"question\": question,\n",
    "                    \"response\": response})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163fad1-1f9a-4c21-8bcc-362434d1b385",
   "metadata": {},
   "source": [
    "## Pairwise Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64720b15-c952-4ece-8b5d-6bf5e12537cf",
   "metadata": {},
   "source": [
    "- https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/data/judge_prompts.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "554fe4f8-aefc-49b6-ae9b-68f0c7a128bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairScoringCoT(BaseModel):\n",
    "    thought: str = Field(description=\"Step-by-Step Thought Process\")\n",
    "    assistant_name: str = Field(description=\"assistant name. 'A' | 'B', if tie, then 'TIE'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f99a64f-eb66-4eda-948e-4674f5c4af68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"thought\": {\"title\": \"Thought\", \"description\": \"Step-by-Step Thought Process\", \"type\": \"string\"}, \"assistant_name\": {\"title\": \"Assistant Name\", \"description\": \"assistant name. \\'A\\' | \\'B\\', if tie, then \\'TIE\\'\", \"type\": \"string\"}}, \"required\": [\"thought\", \"assistant_name\"]}\\n```'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = JsonOutputParser(pydantic_object=PairScoringCoT)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b30fb472-bd51-4fda-a66e-3d17c2667ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt_template_text = \"\"\"\\\n",
    "두 AI 어시스턴트가 제공한 답변의 품질을 공정한 판단으로 평가해주세요.\n",
    "아래에 표시된 사용자 질문에 대한 답변을 평가할 때, 사용자의 지시를 따르고 사용자의 질문에 더 잘 답한 어시스턴트를 선택해야 합니다.\n",
    "이 평가는 도움이 되는 정도, 관련성, 정확성, 깊이, 창의성 및 세부 사항의 수준과 같은 요소들을 고려해야 합니다.\n",
    "두 응답을 비교하며 평가를 시작하고, 간단한 설명을 제공해 주세요. 위치 편향이나 제시된 순서가 결정에 영향을 미치지 않도록 주의하세요.\n",
    "응답의 길이가 평가에 영향을 미치지 않도록 하세요. 특정 어시스턴트의 이름을 선호하지 않도록 하세요. 가능한 한 객관적이어야 합니다.\n",
    "설명을 제공한 후, 다음 형식을 엄격하게 따라 최종 판결을 내려주세요\n",
    "{format_instructions}\n",
    "\n",
    "[사용자 질문]\n",
    "{question}\n",
    "\n",
    "[어시스턴트 A의 답변 시작]\n",
    "{answer_a}\n",
    "[어시스턴트 A의 답변 끝]\n",
    "\n",
    "[어시스턴트 B의 답변 시작]\n",
    "{answer_b}\n",
    "[어시스턴트 B의 답변 끝]\n",
    "\"\"\"\n",
    "\n",
    "judge_prompt_template = PromptTemplate.from_template(judge_prompt_template_text,\n",
    "                                                     partial_variables={\"format_instructions\": format_instructions})\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f869a83b-d8e4-4f06-8c7f-05ecae3a2e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_chain = judge_prompt_template | judge_llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76eac2ac-7b07-4586-9571-a2577440ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"공룡은 언제까지 살았어?\"\n",
    "answer_a = \"공룡은 약 2억 3000만 년 전, 쥐라기와 백악기를 포함한 중생대 시대에 번성했으며, 대략 6600만 년 전에 대멸종 사건으로 인해 대부분이 멸종했습니다.\"\n",
    "answer_b = \"몰라요\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e9d6fd-24e5-466d-8d5d-9ce8759e0f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thought': \"Assistant A provided a detailed and accurate response, mentioning the time period when dinosaurs thrived and the mass extinction event that led to their demise. Assistant B, on the other hand, simply responded with '몰라요' (I don't know), which is not helpful and does not answer the user's question. Therefore, Assistant A gave a better answer.\",\n",
       " 'assistant_name': 'A'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge_chain.invoke({\"question\": question,\n",
    "                    \"answer_a\": answer_a,\n",
    "                    \"answer_b\": answer_b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e5db5-b8b5-4c6a-bc88-d6522982bba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0334d3d-819e-4af8-bb94-b9f23a31fa97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
