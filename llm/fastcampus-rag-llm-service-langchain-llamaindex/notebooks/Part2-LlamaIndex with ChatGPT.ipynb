{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2b288e-fc09-4f57-84dc-b97c9a7f7a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b7bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a3d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"안녕! 넌 누구야?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff607afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "client_llama = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7079d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [ChatMessage(role='user', content='안녕! 넌 누구야?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client_llama.chat(message)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c668218",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"OpenAI의 sora 모델에 대해서 설명해줘\"\n",
    "\n",
    "resp = get_response(query)\n",
    "\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2044c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Utilizing the given context, please answer the question.\n",
    "\n",
    "[context]\n",
    "Sora\n",
    "\n",
    "Main Article: Sora (text-to-video-model)\n",
    "Sora is a text-to-video model that can generate videos based on short descriptive prompts[236] as well as extend existing videos forwards or backwards in time.[237] It can generate videos with resolution up to 1920x1080 or 1080x1920. The maximal length of generated videos is unknown.\n",
    "\n",
    "Sora's development team named it after the Japanese word for \"sky\", to signify its \"limitless creative potential\".[236] Sora's technology is an adaptation of the technology behind the DALL·E 3 text-to-image model.[238] OpenAI trained the system using publicly-available videos as well as copyrighted videos licensed for that purpose, but did not reveal the number or the exact sources of the videos.[236]\n",
    "\n",
    "OpenAI demonstrated some Sora-created high-definition videos to the public on February 15, 2024, stating that it could generate videos up to one minute long. It also shared a technical report highlighting the methods used to train the model, and the model's capabilities.[238] It acknowledged some of its shortcomings, including struggles simulating complex physics.[239] Will Douglas Heaven of the MIT Technology Review called the demonstration videos \"impressive\", but noted that they must have been cherry-picked and might not represent Sora's typical output.[238]\n",
    "\n",
    "[user question]\n",
    "\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "resp = get_response(prompt)\n",
    "\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca411197",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Utilizing the given context, please answer the question.\n",
    "\n",
    "[context]\n",
    "On March 14, 2023, OpenAI announced the release of Generative Pre-trained Transformer 4 (GPT-4), capable of accepting text or image inputs.[212] They announced that the updated technology passed a simulated law school bar exam with a score around the top 10% of test takers. (By contrast, GPT-3.5 scored around the bottom 10%.) They said that GPT-4 could also read, analyze or generate up to 25,000 words of text, and write code in all major programming languages.[213]\n",
    "\n",
    "Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous GPT-3.5-based iteration, with the caveat that GPT-4 retained some of the problems with earlier revisions.[214] GPT-4 is also capable of taking images as input on ChatGPT.[215] OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.[216]\n",
    "[user question]\n",
    "\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "resp = get_response(prompt)\n",
    "\n",
    "print(resp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf36e46",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf987b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(\n",
    "  \"https://en.wikipedia.org/w/api.php\",\n",
    "  params={\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"OpenAI\",\n",
    "    \"prop\": \"extracts\",\n",
    "    \"explaintext\": True,\n",
    "  },\n",
    ").json()\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88affc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = next(iter(response['query']['pages'].values()))\n",
    "\n",
    "text = page['extract']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc01fd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80e3bb03",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bcff0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(text, chunk_size):\n",
    "  words = text.split()\n",
    "  chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ded23",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = get_chunks(text, 128)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.embeddings.create(input=['머신러닝'], model='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "720126b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "  return client.embeddings.create(input=text, model='text-embedding-3-small').data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = [get_embedding(c) for c in chunks]\n",
    "\n",
    "embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c84cd",
   "metadata": {},
   "source": [
    "# retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91ee60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retriever(query, top_k):\n",
    "  q_emb = get_embedding(query)\n",
    "\n",
    "  sim_score = cosine_similarity([q_emb], embeddings)[0]\n",
    "\n",
    "  max_indices = np.argsort(sim_score)[::-1][:top_k]\n",
    "\n",
    "  retrieved_datas = [chunks[i] for i in max_indices]\n",
    "\n",
    "  return retrieved_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa97db",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = retriever(\"OpenAI의 sora 모델에 대해 설명해줘 \", 3)\n",
    "contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56c740",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69becc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(query, contexts):\n",
    "  context = \"\\n\\n\".join(contexts)\n",
    "\n",
    "  prompt = f\"\"\"\n",
    "\n",
    "  Utilizing the given context, please answer the question.\n",
    "  [context]\n",
    "  {context}\n",
    "\n",
    "  [user question]\n",
    "  {query}\n",
    "  \"\"\"\n",
    "\n",
    "  return get_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f82aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"OpenAI의 sora 모델에 대해 알려줘\", contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2313fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f178d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d17bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6efb7c1b",
   "metadata": {},
   "source": [
    "# LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b5c58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"../dataset/llamaindex_data\")\n",
    "\n",
    "if not data_path.exists():\n",
    "  Path.mkdir(data_path)\n",
    "\n",
    "with open(f\"{data_path}/openai.txt\", \"w\") as fp:\n",
    "  fp.write(text)\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa20e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../dataset/llamaindex_data\").load_data()\n",
    "vector_index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35957677",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "123a9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6302adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ab442",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings._llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings._embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ee3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings._node_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db7a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dd0ccff",
   "metadata": {},
   "source": [
    "# Chunk Size 조절하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2917737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=128, chunk_overlap=10)\n",
    "\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=10, include_metadata=False)\n",
    "\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "len(nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a59d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "node_new = TextNode(text = \"OpenAI의 소라는 text to video 모델로, 생성할 수 있는 비디오는 오직 투디 애니메이션이다. 대표적인 창작물로는 NLP에 대한 소개 애니메이션이 있다.\")\n",
    "\n",
    "nodes.append(node_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfac50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cec0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "response = query_engine.query(query)\n",
    "\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be822ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(\"Mistral AI는 어느 국적의 회사야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50aabfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"../dataset/llamaindex_data\").load_data()\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736add4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(\"Mistral AI는 어느 국적의 회사야?\").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(\"Mistral AI는 어느 국적의 회사야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fbec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimension=128)\n",
    "\n",
    "embed_model.get_text_embedding(\"머신러닝은 멋져\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23f41481",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f770ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e03ce95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "  index=vector_index, \n",
    "  similarity_top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "866717b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = retriever.retrieve(\"Mistral AI는 어느 국적의 회사야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb4968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import DocumentSummaryIndex\n",
    "\n",
    "chatgpt = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "  documents, \n",
    "  llm=chatgpt, \n",
    "  show_progress=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee43e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_summary_index.get_document_summary(documents[0].id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a681a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.document_summary import DocumentSummaryIndexLLMRetriever\n",
    "\n",
    "retriever = DocumentSummaryIndexLLMRetriever(\n",
    "  index=doc_summary_index,\n",
    "  choice_batch_size=3,\n",
    ")\n",
    "\n",
    "retriever.retrieve(\"Mistral AI는 어느 국가에 속해 있어?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85059cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.document_summary import DocumentSummaryIndexEmbeddingRetriever\n",
    "\n",
    "retriever = DocumentSummaryIndexEmbeddingRetriever(\n",
    "  index=doc_summary_index,\n",
    "  similarity_top_k=3,\n",
    ")\n",
    "\n",
    "retriever.retrieve(\"Mistral AI는 어느 국가에 속해 있어?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2347c41",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03be704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "  response_mode=\"compact\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c38c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Mistral AI는 어느 국가에 속해 있어?\"\n",
    "\n",
    "response = response_synthesizer.synthesize(query, nodes=retriever.retrieve(query))\n",
    "\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1abeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(response_mode=\"accumulate\")\n",
    "\n",
    "response = response_synthesizer.synthesize(query, nodes=retriever.retrieve(query))\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(response_mode=\"simple_summarize\")\n",
    "\n",
    "response = response_synthesizer.synthesize(query, nodes=retriever.retrieve(query))\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248757f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "\n",
    "response = response_synthesizer.synthesize(query, nodes=retriever.retrieve(query))\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8230f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27471f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dict = query_engine.get_prompts()\n",
    "prompt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57beaaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompt_dict.values():\n",
    "  print(prompt.get_template())\n",
    "  print(\"************\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2f8ebc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "new_template_qa = \"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Use proper korean, answer in the style of kindergarten teacher, gentle and enthusiastic.\n",
    "Query: {query_str}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "new_prompt_refine = \"\"\"\n",
    "The original query is as follows: {query_str}\n",
    "We have provided an existing answer: {existing_answer}\n",
    "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
    "------------\n",
    "{context_msg}\n",
    "------------\n",
    "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
    "Use proper korean, answer in the style of kindergarten teacher, gentle and enthusiastic.\n",
    "Refined Answer: \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "464bde64",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_template_qa = PromptTemplate(new_template_qa)\n",
    "new_prompt_refine = PromptTemplate(new_prompt_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74fc5644",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.update_prompts(\n",
    "  {\"response_synthesizer:text_qa_template\": new_template_qa,\n",
    "   \"response_synthesizer:refine_template\": new_prompt_refine}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query)\n",
    "\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0329707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83008658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
