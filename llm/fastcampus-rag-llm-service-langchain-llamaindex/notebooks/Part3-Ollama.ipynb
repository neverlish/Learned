{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annyeonghaseyo, dear students! Today, we're going to learn about something very cool called \"Machine Learning\"!\n",
      "\n",
      "Machine Learning, or \"gwaee-jang-sik-jeok\" in Korean, is like teaching a robot or computer how to play our favorite game, \"Chopstick\". You know, when you practice and practice, the robot gets better and better at picking up rice balls?\n",
      "\n",
      "It's kind of like that! When we give computers lots of examples and tell them what we want them to do, they can learn and get better at doing it all by themselves. Isn't that amazing?\n",
      "\n",
      "Imagine we have a toy box full of different blocks, each block with a different color or shape. We show the computer many pictures of these blocks and say \"this one is red\", \"that one is square\". The computer learns from all those examples and can now recognize which ones are red and which ones are square!\n",
      "\n",
      "Machine Learning helps us create special computers that can learn and do new things on their own, just like how we practice and get better at our favorite games!\n",
      "\n",
      "Isn't Machine Learning just the most wonderful thing?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"teacher\")\n",
    "\n",
    "query = \"What is Machine Learning?\"\n",
    "\n",
    "result = llm.invoke(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\"../dataset/llamaindex_data\", glob=\"*\", show_progress=True)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "  chunk_size=500,\n",
    "  chunk_overlap=20,\n",
    "  length_function=len,\n",
    "  is_separator_regex=False,\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "embed_model = OllamaEmbeddings(\n",
    "  model=\"nomic-embed-text:latest\",\n",
    ")\n",
    "\n",
    "vector_index = FAISS.from_documents(\n",
    "  documents,\n",
    "  embed_model,\n",
    ")\n",
    "\n",
    "retriever = vector_index.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "template = \"\"\"\n",
    "Utilizing the context given below, answer the question\n",
    "\n",
    "[context]\n",
    "{context}\n",
    "\n",
    "question: {query}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel({'context': retriever, \"query\": RunnablePassthrough()}) | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hwan-ah! (Hello!) Today, we're going to learn about something new!\n",
      "\n",
      "From what I understand, there's no mention of \"TD Learning\" in the text provided. However, I think you might be referring to \"RL algorithms and training code,\" which is mentioned as a part of Dactyl.\n",
      "\n",
      "But don't worry! Let me try again. In Korean, we have a phrase called \"trial-and-error learning\" or \"learning through trial and error.\" It's when someone tries different things until they find the right solution.\n",
      "\n",
      "So, I think you might be looking for that kind of learning? (wink) We can call it \"TD Learning,\" but in this case, it's actually a way of learning called \"trial-and-error learning\" or \"RL algorithms and training code.\"\n",
      "\n",
      "Does that make sense?\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(\"What is TD Learning?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
