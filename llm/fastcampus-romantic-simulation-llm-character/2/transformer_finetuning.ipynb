{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132e86a9-9639-42df-ac6c-5daa5ff405cf",
   "metadata": {},
   "source": [
    "### gpu 활용 가능한지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceac399-041c-432f-bf3b-4f07efe407d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5763abb-911c-486c-9265-9c5563dea8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 이름 체크(cuda:0에 연결된 그래픽 카드 기준)\n",
    "print(torch.cuda.get_device_name(device = 0)) # 'NVIDIA TITAN X (Pascal)'\n",
    "\n",
    "# 사용 가능 GPU 개수 체크\n",
    "print(torch.cuda.device_count()) # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5970df-5b48-4ad7-b3f8-2909a8a9d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ceeae1-a546-4952-b3b0-ff4eeffb275b",
   "metadata": {},
   "source": [
    "### 필요한 파이썬 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc021bfb-83cf-4b17-9db1-112719f5c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets numpy accelerate pandas tokenizers matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e4327-6ca1-4fa9-9c3d-31ca5650d16a",
   "metadata": {},
   "source": [
    "### 워드 토큰화 및 임베딩 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c367bf-99cd-4295-aaeb-ecfb8277fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\", force_download=True)\n",
    "\n",
    "sentence = \"안녕하세요. 오늘은 LLM 파인 튜닝을 배워볼게요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590752d2-724c-437b-a672-c480321e3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3d39a-db77-4ef1-8b38-43f26c2d1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdec8b1-f679-4a36-8a37-eea1e5ccf8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스로 변환 (토큰 ID)\n",
    "token_ids = tokenizer.encode(sentence)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e38e6c-dfcf-474d-8fd3-61c8a0dd0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0317b-2ade-49ba-b672-2b7be272e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 한국어 모델 로드 (KoBERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n",
    "\n",
    "# 한국어 문장 토큰화\n",
    "sentence = \"안녕하세요, 한국어 LLM 실습입니다.\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# 토큰 인덱스로 변환\n",
    "token_ids = tokenizer.encode(sentence)\n",
    "print(\"Token IDs:\", token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac91ae5-579f-4100-bb62-233ae8e4bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a457872d-627b-4d8a-9d5d-1bbb50c33ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "# Custom Callback to log every step\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            print(f\"Step: {state.global_step}, Training Loss: {logs.get('loss', 'N/A')}\")\n",
    "\n",
    "# 한국어 데이터셋 로드 (NSMC 영화 리뷰 데이터셋)\n",
    "dataset = load_dataset('nsmc')\n",
    "\n",
    "# 사전 학습된 한국어 BERT 모델 로드 및 GPU로 이동\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"beomi/kcbert-base\", num_labels=2).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n",
    "\n",
    "# 토큰화 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['document'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 훈련 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"steps\",       # 매 스텝마다 평가\n",
    "    save_strategy=\"steps\",             # 매 스텝마다 모델 저장\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',              # 로그 디렉토리\n",
    "    logging_steps=10,                  # 로그 출력 간격\n",
    "    log_level=\"info\",                  # 로그 레벨 설정\n",
    "    save_total_limit=2,                # 모델 저장 횟수 제한\n",
    "    save_steps=500,                    # 500 스텝마다 모델 저장\n",
    "    eval_steps=500,                    # 500 스텝마다 평가\n",
    "    load_best_model_at_end=True,       # 학습 종료 시 베스트 모델 로드\n",
    "    fp16=True,                         # Mixed precision 학습\n",
    "    report_to=\"none\",                  # 콘솔에 로그 출력\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    callbacks=[CustomCallback()],  # 콜백 추가\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(\"./bert_korean_model\")\n",
    "tokenizer.save_pretrained(\"./bert_korean_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91431563-d9fd-462d-972c-ce9cc905a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# GPT 모델 및 토크나이저 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "\n",
    "# 한국어 데이터셋 로드 (NSMC 예시)\n",
    "dataset = load_dataset(\"nsmc\")\n",
    "\n",
    "# 토큰화 및 패딩 설정\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['document'], padding=\"max_length\", truncation=True, max_length=128)  # 고정된 시퀀스 길이 설정\n",
    "\n",
    "# 토큰화된 데이터셋 생성\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 모델 입력에 맞는 데이터셋 형식으로 변환\n",
    "def group_texts(examples):\n",
    "    # GPT 모델은 시퀀스가 연결된 상태로 학습되므로 여러 텍스트를 하나의 시퀀스로 그룹화\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    if total_length >= 128:  # 여기서는 max_length와 일치해야 함\n",
    "        total_length = (total_length // 128) * 128\n",
    "    result = {k: [t[i: i + 128] for i in range(0, total_length, 128)] for k, t in concatenated_examples.items()}\n",
    "    return result\n",
    "\n",
    "# 데이터셋을 GPT 모델 입력에 맞게 변환\n",
    "tokenized_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "\n",
    "# 훈련 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt_korean_results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./gpt_logs',\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Mixed precision 학습 (FP16) GPU 속도 향상\n",
    ")\n",
    "\n",
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    ")\n",
    "\n",
    "# 파인튜닝 시작 (GPU에서 실행)\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(\"./gpt_korean_model\")\n",
    "tokenizer.save_pretrained(\"./gpt_korean_model\")\n",
    "\n",
    "\n",
    "# 다음 단어 예측 실습\n",
    "input_sentence = \"오늘 날씨는\"\n",
    "inputs = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50, do_sample=True, top_k=50)\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"생성된 문장:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a1217-1aaa-4b64-8460-fbdb4d0999d1",
   "metadata": {},
   "source": [
    "bert pretrained model 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b2d6f-3051-4c52-97c4-048085b5f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 사전 학습된 한국어 BERT 모델 로드 (분류용)\n",
    "model_name = \"beomi/kcbert-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPU 설정 (필요 시 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 예측할 문장\n",
    "input_sentence = \"행복\"\n",
    "inputs = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 모델 예측\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    print(logits)\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(f\"예측 결과: {'긍정' if predicted_class == 1 else '부정'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad80fa-25c7-49ad-a18d-9f609bc307c2",
   "metadata": {},
   "source": [
    "gpt pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9e69b-e2f5-4011-bdb7-6ba0ad99eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 사전 학습된 GPT 모델 로드 (텍스트 생성용)\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPU 설정 (필요 시 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 문장 입력\n",
    "input_sentence = \"오늘 날씨는\"\n",
    "inputs = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 텍스트 생성\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_length=100, do_sample=True, top_k=50)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 생성된 텍스트 출력\n",
    "print(\"생성된 텍스트:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad1678-a667-4488-8c15-7e0cc1ed0eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
