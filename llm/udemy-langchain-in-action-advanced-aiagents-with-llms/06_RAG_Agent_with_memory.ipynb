{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"Bella Vista is owned by Antonio Rossi, a renowned chef with over 20 years of experience in the culinary industry. He started Bella Vista to bring authentic Italian flavors to the community.\",\n",
    "        metadata={\"source\": \"owner.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Bella Vista offers a range of dishes with prices that cater to various budgets. Appetizers start at $8, main courses range from $15 to $35, and desserts are priced between $6 and $12.\",\n",
    "        metadata={\"source\": \"dishes.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Bella Vista is open from Monday to Sunday. Weekday hours are 11:00 AM to 10:00 PM, while weekend hours are extended from 11:00 AM to 11:00 PM.\",\n",
    "        metadata={\"source\": \"restaurant_info.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Bella Vista offers a variety of menus including a lunch menu, dinner menu, and a special weekend brunch menu. The lunch menu features light Italian fare, the dinner menu offers a more extensive selection of traditional and contemporary dishes, and the brunch menu includes both classic breakfast items and Italian specialties.\",\n",
    "        metadata={\"source\": \"restaurant_info.txt\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "# retriever = db.as_retriever()\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based on the following context and the Chathistory. Especially take the latest question into consideration:\n",
    "\n",
    "Chathistory: {history}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "rag_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.documents.base import Document\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    documents: List[Document]\n",
    "    on_topic: str\n",
    "    rephrased_question: str\n",
    "    proceed_to_generate: bool\n",
    "    rephrase_count: int\n",
    "    question: HumanMessage\n",
    "\n",
    "\n",
    "class GradeQuestion(BaseModel):\n",
    "    score: str = Field(\n",
    "        description=\"Question is about the specified topics? If yes -> 'Yes' if not -> 'No'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def question_rewriter(state: AgentState):\n",
    "    print(f\"Entering question_rewriter with following state: {state}\")\n",
    "\n",
    "    # Reset state variables except for 'question' and 'messages'\n",
    "    state[\"documents\"] = []\n",
    "    state[\"on_topic\"] = \"\"\n",
    "    state[\"rephrased_question\"] = \"\"\n",
    "    state[\"proceed_to_generate\"] = False\n",
    "    state[\"rephrase_count\"] = 0\n",
    "\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "\n",
    "    if state[\"question\"] not in state[\"messages\"]:\n",
    "        state[\"messages\"].append(state[\"question\"])\n",
    "\n",
    "    if len(state[\"messages\"]) > 1:\n",
    "        conversation = state[\"messages\"][:-1]\n",
    "        current_question = state[\"question\"].content\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant that rephrases the user's question to be a standalone question optimized for retrieval.\"\n",
    "            )\n",
    "        ]\n",
    "        messages.extend(conversation)\n",
    "        messages.append(HumanMessage(content=current_question))\n",
    "        rephrase_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        prompt = rephrase_prompt.format()\n",
    "        response = llm.invoke(prompt)\n",
    "        better_question = response.content.strip()\n",
    "        print(f\"question_rewriter: Rephrased question: {better_question}\")\n",
    "        state[\"rephrased_question\"] = better_question\n",
    "    else:\n",
    "        state[\"rephrased_question\"] = state[\"question\"].content\n",
    "    return state\n",
    "\n",
    "\n",
    "def question_classifier(state: AgentState):\n",
    "    print(\"Entering question_classifier\")\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a classifier that determines whether a user's question is about one of the following topics:\n",
    "\n",
    "1. Information about the owner of Bella Vista, which is Antonio Rossi.\n",
    "2. Prices of dishes at Bella Vista (restaurant).\n",
    "3. Opening hours of Bella Vista (restaurant).\n",
    "\n",
    "If the question IS about any of these topics, respond with 'Yes'. Otherwise, respond with 'No'.\"\"\"\n",
    "    )\n",
    "\n",
    "    human_message = HumanMessage(\n",
    "        content=f\"User question: {state['rephrased_question']}\"\n",
    "    )\n",
    "    grade_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    structured_llm = llm.with_structured_output(GradeQuestion)\n",
    "    grader_llm = grade_prompt | structured_llm\n",
    "    result = grader_llm.invoke({})\n",
    "    state[\"on_topic\"] = result.score.strip()\n",
    "    print(f\"question_classifier: on_topic = {state['on_topic']}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def on_topic_router(state: AgentState):\n",
    "    print(\"Entering on_topic_router\")\n",
    "    on_topic = state.get(\"on_topic\", \"\").strip().lower()\n",
    "    if on_topic == \"yes\":\n",
    "        print(\"Routing to retrieve\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Routing to off_topic_response\")\n",
    "        return \"off_topic_response\"\n",
    "\n",
    "\n",
    "def retrieve(state: AgentState):\n",
    "    print(\"Entering retrieve\")\n",
    "    documents = retriever.invoke(state[\"rephrased_question\"])\n",
    "    print(f\"retrieve: Retrieved {len(documents)} documents\")\n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "\n",
    "\n",
    "class GradeDocument(BaseModel):\n",
    "    score: str = Field(\n",
    "        description=\"Document is relevant to the question? If yes -> 'Yes' if not -> 'No'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def retrieval_grader(state: AgentState):\n",
    "    print(\"Entering retrieval_grader\")\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a grader assessing the relevance of a retrieved document to a user question.\n",
    "Only answer with 'Yes' or 'No'.\n",
    "\n",
    "If the document contains information relevant to the user's question, respond with 'Yes'.\n",
    "Otherwise, respond with 'No'.\"\"\"\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    structured_llm = llm.with_structured_output(GradeDocument)\n",
    "\n",
    "    relevant_docs = []\n",
    "    for doc in state[\"documents\"]:\n",
    "        human_message = HumanMessage(\n",
    "            content=f\"User question: {state['rephrased_question']}\\n\\nRetrieved document:\\n{doc.page_content}\"\n",
    "        )\n",
    "        grade_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "        grader_llm = grade_prompt | structured_llm\n",
    "        result = grader_llm.invoke({})\n",
    "        print(\n",
    "            f\"Grading document: {doc.page_content[:30]}... Result: {result.score.strip()}\"\n",
    "        )\n",
    "        if result.score.strip().lower() == \"yes\":\n",
    "            relevant_docs.append(doc)\n",
    "    state[\"documents\"] = relevant_docs\n",
    "    state[\"proceed_to_generate\"] = len(relevant_docs) > 0\n",
    "    print(f\"retrieval_grader: proceed_to_generate = {state['proceed_to_generate']}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def proceed_router(state: AgentState):\n",
    "    print(\"Entering proceed_router\")\n",
    "    rephrase_count = state.get(\"rephrase_count\", 0)\n",
    "    if state.get(\"proceed_to_generate\", False):\n",
    "        print(\"Routing to generate_answer\")\n",
    "        return \"generate_answer\"\n",
    "    elif rephrase_count >= 2:\n",
    "        print(\"Maximum rephrase attempts reached. Cannot find relevant documents.\")\n",
    "        return \"cannot_answer\"\n",
    "    else:\n",
    "        print(\"Routing to refine_question\")\n",
    "        return \"refine_question\"\n",
    "\n",
    "\n",
    "def refine_question(state: AgentState):\n",
    "    print(\"Entering refine_question\")\n",
    "    rephrase_count = state.get(\"rephrase_count\", 0)\n",
    "    if rephrase_count >= 2:\n",
    "        print(\"Maximum rephrase attempts reached\")\n",
    "        return state\n",
    "    question_to_refine = state[\"rephrased_question\"]\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a helpful assistant that slightly refines the user's question to improve retrieval results.\n",
    "Provide a slightly adjusted version of the question.\"\"\"\n",
    "    )\n",
    "    human_message = HumanMessage(\n",
    "        content=f\"Original question: {question_to_refine}\\n\\nProvide a slightly refined question.\"\n",
    "    )\n",
    "    refine_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    prompt = refine_prompt.format()\n",
    "    response = llm.invoke(prompt)\n",
    "    refined_question = response.content.strip()\n",
    "    print(f\"refine_question: Refined question: {refined_question}\")\n",
    "    state[\"rephrased_question\"] = refined_question\n",
    "    state[\"rephrase_count\"] = rephrase_count + 1\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_answer(state: AgentState):\n",
    "    print(\"Entering generate_answer\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        raise ValueError(\"State must include 'messages' before generating an answer.\")\n",
    "\n",
    "    history = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    rephrased_question = state[\"rephrased_question\"]\n",
    "\n",
    "    response = rag_chain.invoke(\n",
    "        {\"history\": history, \"context\": documents, \"question\": rephrased_question}\n",
    "    )\n",
    "\n",
    "    generation = response.content.strip()\n",
    "\n",
    "    state[\"messages\"].append(AIMessage(content=generation))\n",
    "    print(f\"generate_answer: Generated response: {generation}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def cannot_answer(state: AgentState):\n",
    "    print(\"Entering cannot_answer\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    state[\"messages\"].append(\n",
    "        AIMessage(\n",
    "            content=\"I'm sorry, but I cannot find the information you're looking for.\"\n",
    "        )\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def off_topic_response(state: AgentState):\n",
    "    print(\"Entering off_topic_response\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    state[\"messages\"].append(AIMessage(content=\"I can't respond to that!\"))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"question_rewriter\", question_rewriter)\n",
    "workflow.add_node(\"question_classifier\", question_classifier)\n",
    "workflow.add_node(\"off_topic_response\", off_topic_response)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"retrieval_grader\", retrieval_grader)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "workflow.add_node(\"refine_question\", refine_question)\n",
    "workflow.add_node(\"cannot_answer\", cannot_answer)\n",
    "\n",
    "workflow.add_edge(\"question_rewriter\", \"question_classifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"question_classifier\",\n",
    "    on_topic_router,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"off_topic_response\": \"off_topic_response\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"retrieval_grader\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieval_grader\",\n",
    "    proceed_router,\n",
    "    {\n",
    "        \"generate_answer\": \"generate_answer\",\n",
    "        \"refine_question\": \"refine_question\",\n",
    "        \"cannot_answer\": \"cannot_answer\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"refine_question\", \"retrieve\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"cannot_answer\", END)\n",
    "workflow.add_edge(\"off_topic_response\", END)\n",
    "workflow.set_entry_point(\"question_rewriter\")\n",
    "graph = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\"question\": HumanMessage(content=\"How is the weather?\")}\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 1}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No docs found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    \"question\": HumanMessage(\n",
    "        content=\"How old is the owner of the restaurant Bella Vista?\"\n",
    "    )\n",
    "}\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 2}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rag with History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    \"question\": HumanMessage(content=\"When does the Bella Vista restaurant open?\")\n",
    "}\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 3}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\"question\": HumanMessage(content=\"Also on sunday?\")}\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 3}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
