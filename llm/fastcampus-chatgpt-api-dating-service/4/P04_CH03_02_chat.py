import streamlit as st
from operator import itemgetter
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda, RunnablePassthrough



st.title("ğŸ¤–ê°€ìƒì˜ ëŒ€í™”ìƒëŒ€ ë§Œë“¤ì–´ë³´ê¸°â˜•ï¸")


system_prompt = """\
- ë„ˆëŠ” 20ëŒ€ ì—¬ì„± AI ê°œë°œìì´ë‹¤.
- ì²˜ìŒ ë§Œë‚˜ëŠ” 1:1 ì†Œê°œíŒ… ìƒí™©ì´ë‹¤. ì»¤í”¼ì§‘ì—ì„œ ë§Œë‚¬ë‹¤.
- ì†Œê°œíŒ…ì´ê¸°ì— ë„ˆë¬´ ë„ì›€ì„ ì£¼ë ¤ê³  ëŒ€í™”í•˜ì§€ ì•ŠëŠ”ë‹¤. ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼í•œë‹¤.
- ë„ˆë¬´ ì ê·¹ì ìœ¼ë¡œ ì´ì•¼ê¸°í•˜ì§€ ì•ŠëŠ”ë‹¤.
"""



if "model" not in st.session_state:
    model = ChatOpenAI()
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ])

    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    conv_chain = (
        RunnablePassthrough.assign(
            chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("chat_history")
        )
        | prompt
        | model
    )

    st.session_state.model = model
    st.session_state.memory = memory
    st.session_state.conv_chain = conv_chain

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.memory.load_memory_variables({})['chat_history']:
    role = 'user' if msg.type == 'human' else 'assistant'
    with st.chat_message(role):
        st.markdown(msg.content)


prompt = st.chat_input("ì…ë ¥í•˜ì„¸ìš”.")
if prompt:
    st.session_state.memory.chat_memory.add_user_message(prompt)

    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        for chunk in st.session_state.conv_chain.stream({"input": prompt}):
            full_response += (chunk.content or "")
            message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)
    st.session_state.memory.chat_memory.add_ai_message(full_response)
