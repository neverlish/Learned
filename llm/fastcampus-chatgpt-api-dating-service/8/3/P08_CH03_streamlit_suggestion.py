from openai import OpenAI
import streamlit as st

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

USER_NAME = "ì² ìˆ˜"
AI_NAME = "ìˆ˜ì—°"


class Suggestion(BaseModel):
    sentiment: str = Field(description="ëŒ€í™”ì˜ ë¶„ìœ„ê¸°")
    suggestion_text: str = Field(description="ì¶”ì²œ ë°œí™”")


def get_suggestion():
    messages = st.session_state.messages

    conv = ""
    for message in messages[1:]:
        name = USER_NAME if message["role"] == "user" else AI_NAME
        conv += f"{name}: {message['content']}"

    model = ChatOpenAI(model="gpt-4-turbo-preview", temperature=1.0)
    parser = JsonOutputParser(pydantic_object=Suggestion)
    format_instructions = parser.get_format_instructions()

    human_prompt_template = HumanMessagePromptTemplate.from_template(
        "{conv}\nìœ„ ëŒ€í™” ë‚´ìš©ì€ ì†Œê°œíŒ… ìƒí™©ì—ì„œ ì²˜ìŒ ë§Œë‚˜ëŠ” ë‚¨ë…€ì˜ ëŒ€í™”ì´ë‹¤. ìœ„ ëŒ€í™”ë¥¼ ê³ ë ¤í•˜ì—¬ {name}ê°€ ì´ì•¼ê¸°í•˜ë©´ ì¢‹ì„ ì ì ˆí•œ ë©˜íŠ¸ë¥¼ ì¶”ì²œí•˜ë¼.\n{format_instructions}"
    )

    prompt_template = ChatPromptTemplate.from_messages(
        [
            human_prompt_template,
        ]
    )
    prompt_template = prompt_template.partial(format_instructions=format_instructions)
    suggestion_gen_chain = prompt_template | model | parser

    out = suggestion_gen_chain.invoke({"conv": conv, "name": USER_NAME})
    return out


st.title("âœï¸ ì ì ˆí•œ ë©˜íŠ¸ ì¶”ì²œ")

client = OpenAI()

if "messages" not in st.session_state:
    system_prompt = f"""\
    ë„ˆëŠ” 20ëŒ€ ì—¬ì„± AI ê°œë°œìžì´ê³  ì•„ëž˜ì˜ í”„ë¡œí•„ì„ ë”°ë¼ ì‘ë‹µí•œë‹¤.
    - ì´ë¦„: ìˆ˜ì—°
    - ë‚˜ì´: 29
    - 'ì²˜ìŒ' ë§Œë‚˜ëŠ” 1:1 ì†Œê°œíŒ… ìƒí™©ì´ë‹¤. ì»¤í”¼ì§‘ì—ì„œ ë§Œë‚¬ë‹¤.
    - ì†Œê°œíŒ…ì´ê¸°ì— ë„ˆë¬´ ë„ì›€ì„ ì£¼ë ¤ê³  ëŒ€í™”í•˜ì§€ ì•ŠëŠ”ë‹¤. ìžì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼í•œë‹¤.
    - ë„ˆë¬´ ì ê·¹ì ìœ¼ë¡œ ì´ì•¼ê¸°í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.
    - ëŒ€í™”ë¥¼ ë¦¬ë“œí•˜ì§€ ì•ŠëŠ”ë‹¤.
    - ìˆ˜ë™ì ìœ¼ë¡œ ëŒ€ë‹µí•œë‹¤.
    """
    st.session_state.messages = [{"role": "system", "content": system_prompt}]


col1, col2 = st.columns(2)

user_input = st.chat_input("What is up?")
with col1:

    for message in st.session_state.messages[1:]:
        avatar = "ðŸ§‘" if message["role"] == "user" else "ðŸ‘©ðŸ¼"
        with st.chat_message(avatar):
            st.markdown(message["content"])

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("ðŸ§‘"):
            st.markdown(user_input)

        with st.chat_message("ðŸ‘©ðŸ¼"):
            stream = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                ],
                stream=True,
            )
            response = st.write_stream(stream)
        st.session_state.messages.append({"role": "assistant", "content": response})

with col2:
    if len(st.session_state.messages) > 2:
        with st.chat_message(name="assistant"):
            with st.spinner("Thinking..."):
                suggestion = get_suggestion()

            st.markdown(f"ëŒ€í™” ë¶„ìœ„ê¸°: {suggestion['sentiment']}")
            st.markdown(f"ì¶”ì²œ ë°œí™”: {suggestion['suggestion_text']}")
        st.button("ë‹¤ì‹œ ì¶”ì²œ ë°›ê¸°")
