from openai import OpenAI
import streamlit as st
import pandas as pd

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings


# streamlit window wideë¡œ
st.set_page_config(layout="wide")


if "retriever" not in st.session_state:
    vectorstore = FAISS.load_local(
        folder_path="./conv_index",
        embeddings=OpenAIEmbeddings(),
        allow_dangerous_deserialization=True,
    )
    retriever = vectorstore.as_retriever()

    st.session_state.retriever = retriever


st.header("ðŸ’¬ ë°ì´íŒ… ì–´ì‹œìŠ¤í„´íŠ¸")


USER_NAME = "ì² ìˆ˜"
AI_NAME = "ìˆ˜ì—°"


def get_suggestion():

    model = ChatOpenAI(model="gpt-4-1106-preview", temperature=0.8)
    retriever = st.session_state.retriever

    conv = ""
    for message in st.session_state.messages[1:]:
        name = USER_NAME if message["role"] == "user" else AI_NAME
        conv += f"{name}: {message['content']}"

    template = """
    # í˜„ìž¬ ëŒ€í™”
    {conv}

    # ê´€ë ¨ ìœ ì‚¬ ëŒ€í™”
    {context}

    ê´€ë ¨ ìœ ì‚¬ ëŒ€í™” ì¤‘ í•˜ë‚˜ë¥¼ ì¶œë ¥í•˜ê³  ê·¸ì—  ëŒ€í•œ ì„¤ëª…í•˜ê³ , ê·¸ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬ í˜„ìž¬ ëŒ€í™”ì— ì–´ìš¸ë¦¬ëŠ” ì¡°ì–¸í•´ì¤˜
    """
    prompt = ChatPromptTemplate.from_template(template)

    chain = (
        {"context": retriever, "conv": RunnablePassthrough()}
        | prompt
        | model
        | StrOutputParser()
    )

    suggestion = chain.stream(conv)

    return suggestion


client = OpenAI()

if "messages" not in st.session_state:
    system_prompt = f"""\
    ë„ˆëŠ” 20ëŒ€ ì—¬ì„± AI ê°œë°œìžì´ê³  ì•„ëž˜ì˜ í”„ë¡œí•„ì„ ë”°ë¼ ì‘ë‹µí•œë‹¤.
    - ì´ë¦„: ìˆ˜ì—°
    - ë‚˜ì´: 29
    - 'ì²˜ìŒ' ë§Œë‚˜ëŠ” 1:1 ì†Œê°œíŒ… ìƒí™©ì´ë‹¤. ì»¤í”¼ì§‘ì—ì„œ ë§Œë‚¬ë‹¤.
    - ì†Œê°œíŒ…ì´ê¸°ì— ë„ˆë¬´ ë„ì›€ì„ ì£¼ë ¤ê³  ëŒ€í™”í•˜ì§€ ì•ŠëŠ”ë‹¤. ìžì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼í•œë‹¤.
    - ë„ˆë¬´ ì ê·¹ì ìœ¼ë¡œ ì´ì•¼ê¸°í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.
    - ëŒ€í™”ë¥¼ ë¦¬ë“œí•˜ì§€ ì•ŠëŠ”ë‹¤.
    - ìˆ˜ë™ì ìœ¼ë¡œ ëŒ€ë‹µí•œë‹¤.
    """
    st.session_state.messages = [{"role": "system", "content": system_prompt}]


user_input = st.chat_input("What is up?")

col1, col2 = st.columns(2)

with col1:
    for message in st.session_state.messages[1:]:
        avatar = "ðŸ§‘" if message["role"] == "user" else "ðŸ‘©ðŸ¼"
        with st.chat_message(avatar):
            st.markdown(message["content"])

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("ðŸ§‘"):
            st.markdown(user_input)

        with st.chat_message("ðŸ‘©ðŸ¼"):
            stream = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                ],
                stream=True,
            )
            response = st.write_stream(stream)
        st.session_state.messages.append({"role": "assistant", "content": response})

with col2:
    if len(st.session_state.messages) > 2:
        with st.spinner("ìœ ì‚¬ ëŒ€í™” ê²€ìƒ‰ì¤‘..."):
            st.write_stream(get_suggestion())
