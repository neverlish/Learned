from openai import OpenAI
import streamlit as st
import pandas as pd

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field


# streamlit window wideë¡œ
st.set_page_config(layout="wide")


st.header("ğŸ’¬ ë°ì´íŒ… ì–´ì‹œìŠ¤í„´íŠ¸")


USER_NAME = "ì² ìˆ˜"
AI_NAME = "ìˆ˜ì—°"


def get_suggestion(messages, num_candi=5):

    conv = ""
    for message in messages[1:]:
        name = USER_NAME if message["role"] == "user" else AI_NAME
        conv += f"{name}: {message['content']}"

    eval_model = ChatOpenAI(
        model="gpt-4-1106-preview", temperature=0.8
    )  # CoT ëŠ” ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ë§Œë“¤ì–´ì•¼í•˜ê¸° ë•Œë¬¸ì— temperatureë¥¼ ì˜¬ë ¤ì•¼í•¨
    basic_model = ChatOpenAI(
        model="gpt-3.5-turbo", temperature=0.8
    )  # CoT ëŠ” ë‹¤ì–‘í•œ ìƒ˜í”Œì„ ë§Œë“¤ì–´ì•¼í•˜ê¸° ë•Œë¬¸ì— temperatureë¥¼ ì˜¬ë ¤ì•¼í•¨

    class Suggestion(BaseModel):
        sentiment: str = Field(description="ëŒ€í™”ì˜ ë¶„ìœ„ê¸°: Positive, Negative, Neutral")
        suggestion_text: str = Field(
            description="ëŒ€í™”ì— ëŒ€í•œ markdowní˜•ì‹ì˜ ìì„¸í•œ ë¶„ì„ê³¼ ì ì ˆí•œ ì¡°ì–¸"
        )

    parser = JsonOutputParser(pydantic_object=Suggestion)
    format_instructions = parser.get_format_instructions()

    human_prompt_template = HumanMessagePromptTemplate.from_template(
        "{conv}\nìœ„ ëŒ€í™” ë‚´ìš©ì€ ì†Œê°œíŒ… ìƒí™©ì—ì„œ ì²˜ìŒ ë§Œë‚˜ëŠ” ë‚¨ë…€ì˜ ëŒ€í™”ì´ë‹¤. ìœ„ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ê³  {name}ì—ê²Œ markdowní˜•ì‹ìœ¼ë¡œ ì ì ˆí•œ ì¡°ì–¸ì„ í•˜ë¼.\n{format_instructions}"
    )

    suggestion_gen_prompt = ChatPromptTemplate.from_messages(
        [
            human_prompt_template,
        ]
    )
    suggestion_gen_prompt = suggestion_gen_prompt.partial(
        format_instructions=format_instructions
    )

    suggestion_gen_chain = suggestion_gen_prompt | basic_model | parser

    class VoteCoT(BaseModel):
        thought: str = Field(
            description="voting numberë¥¼ ì„ íƒí•œ ì´ìœ ì— ëŒ€í•´ ìì„¸íˆ ë„£ì–´ì£¼ì„¸ìš”."
        )
        voting_num: int = Field(description="voting number")

    parser = JsonOutputParser(pydantic_object=VoteCoT)
    format_instructions = parser.get_format_instructions()

    voting_prompt_template = HumanMessagePromptTemplate.from_template(
        "{conv}\në‹¤ìŒì€ ìœ„ ì†Œê°œíŒ… ëŒ€í™”ì—ì„œ {name}ì—ê²Œ í•˜ë©´ ì¢‹ì„ ì¡°ì–¸ì´ë‹¤. ì•„ë˜ì˜ í›„ë³´ ì¤‘ ê°€ì¥ ì¢‹ì€ ê²ƒì„ ì¶”ë¡  ê³¼ì •ê³¼ í•¨ê»˜ íˆ¬í‘œ ë²ˆí˜¸ë¥¼ ì‘ë‹µí•˜ë¼.\n{candidates}\n{format_instructions}"
    )

    voting_prompt = ChatPromptTemplate.from_messages(
        [
            voting_prompt_template,
        ]
    )
    voting_prompt = voting_prompt.partial(format_instructions=format_instructions)
    voting_chain = voting_prompt | eval_model | parser

    suggestion_list = suggestion_gen_chain.batch(
        [{"conv": conv, "name": USER_NAME}] * num_candi
    )

    yield "## Suggestion candidates\n"
    yield "\n---\n".join(
        [
            f"- {i} th\n- {sug['sentiment']}\n- {sug['suggestion_text']}\n"
            for i, sug in enumerate(suggestion_list)
        ]
    )

    candidates = "\n\n".join(
        [f"í›„ë³´ {i}.\n{suggestion}" for i, suggestion in enumerate(suggestion_list)]
    )
    vote_list = voting_chain.batch(
        [{"conv": conv, "name": USER_NAME, "candidates": candidates}] * num_candi
    )

    df = pd.DataFrame(vote_list)
    yield "## Voting\n"
    yield df
    print(df)

    best_candi_num = df["voting_num"].mode()[0]
    best_suggestion = suggestion_list[best_candi_num]

    yield "## Best Suggestion\n"
    yield f"{best_candi_num} th\n\n{best_suggestion['sentiment']}\n\n{best_suggestion['suggestion_text']}"


client = OpenAI()

if "messages" not in st.session_state:
    system_prompt = f"""\
    ë„ˆëŠ” 20ëŒ€ ì—¬ì„± AI ê°œë°œìì´ê³  ì•„ë˜ì˜ í”„ë¡œí•„ì„ ë”°ë¼ ì‘ë‹µí•œë‹¤.
    - ì´ë¦„: ìˆ˜ì—°
    - ë‚˜ì´: 29
    - 'ì²˜ìŒ' ë§Œë‚˜ëŠ” 1:1 ì†Œê°œíŒ… ìƒí™©ì´ë‹¤. ì»¤í”¼ì§‘ì—ì„œ ë§Œë‚¬ë‹¤.
    - ì†Œê°œíŒ…ì´ê¸°ì— ë„ˆë¬´ ë„ì›€ì„ ì£¼ë ¤ê³  ëŒ€í™”í•˜ì§€ ì•ŠëŠ”ë‹¤. ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼í•œë‹¤.
    - ë„ˆë¬´ ì ê·¹ì ìœ¼ë¡œ ì´ì•¼ê¸°í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.
    - ëŒ€í™”ë¥¼ ë¦¬ë“œí•˜ì§€ ì•ŠëŠ”ë‹¤.
    - ìˆ˜ë™ì ìœ¼ë¡œ ëŒ€ë‹µí•œë‹¤.
    """
    st.session_state.messages = [{"role": "system", "content": system_prompt}]


user_input = st.chat_input("What is up?")

col1, col2 = st.columns(2)
with col1:
    for message in st.session_state.messages[1:]:
        avatar = "ğŸ§‘" if message["role"] == "user" else "ğŸ‘©ğŸ¼"
        with st.chat_message(avatar):
            st.markdown(message["content"])

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("ğŸ§‘"):
            st.markdown(user_input)

        with st.chat_message("ğŸ‘©ğŸ¼"):
            stream = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                ],
                stream=True,
            )
            response = st.write_stream(stream)
        st.session_state.messages.append({"role": "assistant", "content": response})

with col2:

    if len(st.session_state.messages) > 2:
        with st.spinner("ë¶„ì„ì¤‘..."):
            stream = get_suggestion(st.session_state.messages)
            response = st.write_stream(stream)
