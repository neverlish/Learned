from langchain_community.retrievers import BM25Retriever
from langchain_community.embeddings.sentence_transformer import (
    SentenceTransformerEmbeddings,
)
from langchain_community.vectorstores import FAISS
from langchain.retrievers import EnsembleRetriever
import transformers
import torch
import streamlit as st

data = [
    {
        "ê¸°ì—…ëª…": "ì‚¼ì„±ì „ì",
        "ë‚ ì§œ": "2024-03-02",
        "ë¬¸ì„œ ì¹´í…Œê³ ë¦¬": "ì¸ìˆ˜í•©ë³‘",
        "ìš”ì•½": "ì‚¼ì„±ì „ìê°€ HVAC(ëƒ‰ë‚œë°©ê³µì¡°) ì‚¬ì—… ì¸ìˆ˜ë¥¼ íƒ€ì§„ ì¤‘ì´ë©°, ì´ëŠ” ê¸°ì¡´ ê°€ì „ ì‚¬ì—…ì˜ ì•½ì  ë³´ì™„ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.",
        "ì£¼ìš” ì´ë²¤íŠ¸": ["ê¸°ì—… ì¸ìˆ˜í•©ë³‘"],
    },
    {
        "ê¸°ì—…ëª…": "ì‚¼ì„±ì „ì",
        "ë‚ ì§œ": "2024-03-24",
        "ë¬¸ì„œ ì¹´í…Œê³ ë¦¬": "ì¸ìˆ˜í•©ë³‘",
        "ìš”ì•½": "í…ŒìŠ¤íŠ¸ í•˜ë‚˜ ë‘˜ ì…‹",
        "ì£¼ìš” ì´ë²¤íŠ¸": ["ì‹ ì œí’ˆ ì¶œì‹œ"],
    },
    {
        "ê¸°ì—…ëª…": "í˜„ëŒ€ì°¨",
        "ë‚ ì§œ": "2024-04-02",
        "ë¬¸ì„œ ì¹´í…Œê³ ë¦¬": "ì¸ìˆ˜í•©ë³‘",
        "ìš”ì•½": "ì‚¼ì„±ì „ìê°€ HVAC(ëƒ‰ë‚œë°©ê³µì¡°) ì‚¬ì—… ì¸ìˆ˜ë¥¼ íƒ€ì§„ ì¤‘ì´ë©°, ì´ëŠ” ê¸°ì¡´ ê°€ì „ ì‚¬ì—…ì˜ ì•½ì  ë³´ì™„ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.",
        "ì£¼ìš” ì´ë²¤íŠ¸": ["ê¸°ì—… ì¸ìˆ˜í•©ë³‘", "ì‹ ì œí’ˆ ì¶œì‹œ"],
    },
]

doc_list = [item["ìš”ì•½"] for item in data]

bm25_retriever = BM25Retriever.from_texts(
    doc_list, metadatas=[{"source": i} for i in range(len(data))]
)
bm25_retriever.k = 1

embedding = SentenceTransformerEmbeddings(
    model_name="distiluse-base-multilingual-cased-v1"
)
faiss_vectorstore = FAISS.from_texts(
    doc_list, embedding, metadatas=[{"source": i} for i in range(len(data))]
)
faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={"k": 1})

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]
)

model_id = "42dot/42dot_LLM-SFT-1.3B"

pipeline = transformers.pipeline(
    "text-generation", model=model_id, model_kwargs={"torch_dtype": torch.float16}
)

pipeline.model.eval()


def search(query):
    ensemble_docs = ensemble_retriever.invoke(query)
    return ensemble_docs


def sllm_generate(query):

    answer = pipeline(
        query,
        max_new_tokens=50,
        do_sample=True,
        temperature=0.3,
        top_p=0.9,
        repetition_penalty=1.2,
    )
    return answer[0]["generated_text"][len(query) :]


def prompt_and_generate(query):
    docs = [doc for doc in search(query)]
    prompt = f"""ì•„ë˜ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ëœ ë‰´ìŠ¤ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ì‹œì˜¤.

ì§ˆë¬¸: {query}
"""

    for i in range(len(docs)):
        prompt += f"ë‰´ìŠ¤{i+1}\n"
        prompt += f"ìš”ì•½: {docs[i].page_content}\n"
        prompt += "\n"
    prompt += "ë‹µë³€: "

    answer = sllm_generate(prompt)
    return answer


st.title("ğŸ¤– íˆ¬ì ì–´ì‹œìŠ¤í„´íŠ¸")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    response = f"bot: {prompt_and_generate(prompt.strip())}"

    with st.chat_message("assistant"):
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})
