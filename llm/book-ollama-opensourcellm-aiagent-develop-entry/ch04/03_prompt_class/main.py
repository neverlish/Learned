from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, AIMessageChunk
from langchain_ollama import ChatOllama
from typing import Iterable
from langchain_core.runnables import RunnableGenerator

# 1. ë©”ì‹œì§€ ê°ì²´ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
prompt = ChatPromptTemplate.from_messages(
    [
        SystemMessage(content="ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ê¸°ìƒí•™ìì…ë‹ˆë‹¤. ë‹µë³€ì€ 200ì ì´ë‚´ë¡œ í•´ì£¼ì„¸ìš”."),
        MessagesPlaceholder(variable_name="chat_history")
    ]
)

# 2. LLM ëª¨ë¸ ì„¤ì •
llm = ChatOllama(model="qwen3:8b")

# 3. ì¶œë ¥ íŒŒì„œ ì„¤ì •
def replace_word_with_emoji(text: str) -> str:
    return text.replace("íƒœí’", "ğŸŒªï¸ ")

def streaming_parse(chunks: Iterable[AIMessageChunk]) -> Iterable[str]:
    buffer = ""
    for text_chunk in chunks:           # LLMìœ¼ë¡œë¶€í„° ë°›ì€ ê° ì²­í¬ë¥¼ ì²˜ë¦¬
        buffer += text_chunk.content    # í˜„ì¬ ë²„í¼ì— ìƒˆë¡œìš´ ì²­í¬ì˜ ë‚´ìš©ì„ ì¶”ê°€
        while " " in buffer:            # ë²„í¼ì— ê³µë°±ì´ ë°œê²¬ë˜ë©´
            word, buffer = buffer.split(" ", 1)         # ê³µë°± ê¸°ì¤€ ì²« ë‹¨ì–´ì™€ ë‚˜ë¨¸ì§€ë¥¼ ë¶„ë¦¬
            yield replace_word_with_emoji(word) + " "   # ì²« ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•˜ê³  ë¦¬í„´
    if buffer:
        yield replace_word_with_emoji(buffer)

streaming_parser = RunnableGenerator(streaming_parse)

# 4. ì²´ì¸ êµ¬ì„±
chain = prompt | llm | streaming_parser

# 5. ì±„íŒ… ê¸°ë¡ ì €ì¥ìš© ë°°ì—´
chat_history = []

# 6. ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ì²´ì¸ ì‹¤í–‰
while True:
    user_input = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ")
    if user_input.lower() == "exit":
        break

    result = ""
    chat_history.append(HumanMessage(content=user_input))
    for chunk in chain.stream({"chat_history": chat_history}):
        print(chunk, end="", flush=True)
        result += chunk
    chat_history.append(AIMessage(content=result))
    print("\n")