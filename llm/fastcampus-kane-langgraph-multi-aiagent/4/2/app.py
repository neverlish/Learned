import streamlit as st
import os
from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import AIMessage, HumanMessage
from utils.tool_calling_event import invoke_our_graph
import asyncio
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_core.messages import HumanMessage, AIMessage
from typing import List, Optional, Tuple
from agent import create_agent
from langchain.tools.retriever import create_retriever_tool
import uuid
import chromadb

# chromadb ìºì‹œ í´ë¦¬ì–´
chromadb.api.client.SharedSystemClient.clear_system_cache()

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
os.environ['LANCHAIN_API_KEY']=str(os.getenv("LANGCHAIN_API_KEY"))
os.environ['LANGCHAIN_TRACING_V2']='true'
os.environ['LANGCHAIN_ENDPOINT']="https://api.smith.langchain.com"

def initialize_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = [AIMessage(content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")]
    if "graph" not in st.session_state:
        st.session_state.graph = create_agent()
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = None
    if "current_files_hash" not in st.session_state:
        st.session_state.current_files_hash = None

def get_files_hash(uploaded_files):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì˜ í•´ì‹œê°’ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return hash(tuple(f.name + str(f.size) for f in uploaded_files))

def process_files(uploaded_files) -> Tuple[Optional[any], Optional[List[dict]]]:
    """
    ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ retriever toolê³¼ ë¬¸ì„œ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # í˜„ì¬ íŒŒì¼ë“¤ì˜ í•´ì‹œê°’ ê³„ì‚°
    current_hash = get_files_hash(uploaded_files)
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ ë™ì¼í•œ íŒŒì¼ë“¤ì¸ì§€ í™•ì¸
    if (st.session_state.current_files_hash == current_hash and 
        st.session_state.vectorstore is not None):
        st.write("ê¸°ì¡´ vectorstore ì‚¬ìš©")

        retriever = st.session_state.vectorstore.as_retriever()
        test_results = retriever.get_relevant_documents("test query")
        st.write(f"Retriever test results: {len(test_results)} documents found")

        file_names = ", ".join(f.name for f in uploaded_files)
        description = f"Search through the following documents: {file_names}"
        retriever_tool = create_retriever_tool(
            retriever,
            "search_docs",
            description,
        )
        return retriever_tool, st.session_state.docs_info
    st.write("ìƒˆë¡œìš´ vectorstore ìƒì„±")
    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=500, 
        chunk_overlap=0
    )
    
    all_docs = []
    docs_info = []
    
    for uploaded_file in uploaded_files:
        try:
            file_info = {
                "name": uploaded_file.name,
                "type": uploaded_file.type,
                "size": uploaded_file.size
            }
            
            if uploaded_file.type == "text/plain":
                text = uploaded_file.getvalue().decode()
                docs = text_splitter.create_documents(
                    [text],
                    metadatas=[{"source": uploaded_file.name}]
                )
                all_docs.extend(docs)
                docs_info.append(file_info)
                
            elif uploaded_file.type == "application/pdf":
                with open("temp.pdf", "wb") as f:
                    f.write(uploaded_file.getvalue())
                loader = PyPDFLoader("temp.pdf")
                docs = loader.load_and_split()
                for doc in docs:
                    doc.metadata["source"] = uploaded_file.name
                docs = text_splitter.split_documents(docs)
                all_docs.extend(docs)
                docs_info.append(file_info)
                os.remove("temp.pdf")
                
            else:
                st.write(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {uploaded_file.type}")
                continue
                
        except Exception as e:
            st.error(f"Error processing {uploaded_file.name}: {str(e)}")
            continue
    
    if all_docs:
        vectorstore = Chroma.from_documents(
            documents=all_docs,
            embedding=OpenAIEmbeddings(),
            collection_name=str(uuid.uuid4())
        )
        # ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state.vectorstore = vectorstore
        st.session_state.current_files_hash = current_hash
        st.session_state.docs_info = docs_info
        
        retriever = vectorstore.as_retriever()
        file_names = ", ".join(d["name"] for d in docs_info)
        description = f"Search through the following documents: {file_names}"
        
        retriever_tool = create_retriever_tool(
            retriever,
            "search_docs",
            description,
        )
        
        return retriever_tool, docs_info
    return None, None

def format_doc_info(docs_info: List[dict]) -> str:
    """ë¬¸ì„œ ì •ë³´ë¥¼ í¬ë§·íŒ…í•˜ì—¬ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    upload_message = "ğŸ“š ë‹¤ìŒ ë¬¸ì„œë“¤ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤:\n"
    for doc in docs_info:
        file_type = "PDF" if "pdf" in doc["type"].lower() else "Text"
        size_kb = doc["size"] / 1024
        upload_message += f"- {doc['name']} ({file_type}, {size_kb:.1f}KB)\n"
    upload_message += "\nì´ì œ ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"
    return upload_message

# Streamlit ì•± ì‹œì‘
st.title("LangGraph Chatbot")
initialize_session_state()

# OpenAI API í‚¤ í™•ì¸
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— APIí‚¤ë¥¼ ì €ì¥í•´ì£¼ì„¸ìš”.")
    st.stop()

# ì‚¬ì´ë“œë°”ì— íŒŒì¼ ì—…ë¡œë” ë°°ì¹˜
with st.sidebar:
    st.header("ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "Upload documents", 
        type=["txt", "pdf"],
        accept_multiple_files=True,
        help="PDF ë˜ëŠ” TXT íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
    )
    
    if uploaded_files:
        with st.spinner("ğŸ“ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
            retriever_tool, docs_info = process_files(uploaded_files)
            if retriever_tool and docs_info:
                # ìƒˆë¡œìš´ ê·¸ë˜í”„ ìƒì„± ë° ì €ì¥
                st.session_state.graph = create_agent(docs_info, retriever_tool)
                st.success(f"âœ… {len(uploaded_files)}ê°œì˜ ë¬¸ì„œê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                # ë¬¸ì„œ ì—…ë¡œë“œ ì•Œë¦¼ ë©”ì‹œì§€ ì¶”ê°€
                upload_message = format_doc_info(docs_info)
                st.session_state.messages.append(AIMessage(content=upload_message))
            
# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message.type):
        st.write(message.content)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
prompt = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

if prompt:
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.write(prompt)
    
    with st.chat_message("assistant"):
        placeholder = st.container()
        try:
            response = asyncio.run(invoke_our_graph(
                {"messages": st.session_state.messages}, 
                placeholder,
                st.session_state.graph  # í˜„ì¬ ì„¸ì…˜ì˜ ê·¸ë˜í”„ ì „ë‹¬
            ))
            st.session_state.messages.append(AIMessage(content=response))
        except RecursionError as e:
            error_message = f"âš ï¸ ë„ˆë¬´ ë§ì€ ì¬ê·€ í˜¸ì¶œì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            st.error(error_message)
            st.session_state.messages.append(AIMessage(content=error_message))