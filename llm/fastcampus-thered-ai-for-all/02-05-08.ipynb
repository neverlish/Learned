{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsalpJ_KLSSv"
      },
      "source": [
        "# 5교시 3.자연어 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyGt_ra8LSSz"
      },
      "source": [
        "## 1. 텍스트의 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ihFiBANMhNu_",
        "outputId": "9a62d229-5802-43eb-8e69-bdf20aa5f8bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "단어 인덱스:\n",
            " {'커피': 1, '한잔': 2, '어때': 3}\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import numpy as np\n",
        "\n",
        "# 전처리할 텍스트를 정합니다.\n",
        "text = '커피 한잔 어때'\n",
        "\n",
        "# Tokenizer 객체 생성 및 fit_on_texts로 단어 인덱스 학습\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "# texts_to_sequences로 텍스트를 시퀀스로 변환\n",
        "sequences = tokenizer.texts_to_sequences([text])\n",
        "\n",
        "# 단어 인덱스 확인\n",
        "word_index = tokenizer.word_index\n",
        "print(\"\\n단어 인덱스:\\n\", word_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3_amJbMOhNvA",
        "outputId": "26d9a9fb-64a0-4bd5-962c-ff9b3bf5db7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "단어 카운트:\n",
            " OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n",
            "\n",
            "문장 카운트:  3\n",
            "\n",
            "각 단어가 몇 개의 문장에 포함되어 있는가:\n",
            " defaultdict(<class 'int'>, {'단어를': 1, '먼저': 1, '토큰화합니다': 1, '텍스트의': 2, '나누어': 1, '각': 1, '단어로': 1, '토큰화해야': 1, '딥러닝에서': 2, '인식됩니다': 1, '토큰화한': 1, '수': 1, '있습니다': 1, '사용할': 1, '결과는': 1})\n",
            "\n",
            "각 단어에 매겨진 인덱스 값:\n",
            " {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"
          ]
        }
      ],
      "source": [
        "# 단어 빈도수 세기\n",
        "\n",
        "# 전처리하려는 세 개의 문장을 정합니다.\n",
        "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n",
        "       '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n",
        "       '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.',\n",
        "       ]\n",
        "\n",
        "# 토큰화 함수를 이용해 전처리 하는 과정입니다.\n",
        "tokenizer = Tokenizer()            # 토큰화 함수 지정\n",
        "tokenizer.fit_on_texts(docs)       # 토큰화 함수에 문장 적용\n",
        "\n",
        "# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력합니다.\n",
        "print(\"\\n단어 카운트:\\n\", tokenizer.word_counts)\n",
        "\n",
        "# 출력되는 순서는 랜덤입니다.\n",
        "print(\"\\n문장 카운트: \", tokenizer.document_count)\n",
        "print(\"\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n\", tokenizer.word_docs)\n",
        "print(\"\\n각 단어에 매겨진 인덱스 값:\\n\",  tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "79GUVn9bhNvA",
        "outputId": "1f65b591-3aca-432b-c89c-d8df838e2fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "단어 인덱스:\n",
            " {'커피': 1, '한잔': 2, '어때': 3, '오늘': 4, '날씨': 5, '참': 6, '좋네': 7, '옷이': 8, '어울려요': 9}\n",
            "\n",
            "시퀀스:\n",
            " [[1, 2, 3], [4, 5, 6, 7], [8, 9]]\n",
            "\n",
            "패딩된 시퀀스:\n",
            " [[0 1 2 3]\n",
            " [4 5 6 7]\n",
            " [0 0 8 9]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\n",
            "임베딩 결과:\n",
            " [[[ 1.6098980e-02  1.1575662e-02 -3.7366189e-02  3.4891069e-05\n",
            "    2.0950962e-02]\n",
            "  [ 3.2916777e-03 -4.8966706e-02 -5.6972988e-03  4.3367371e-03\n",
            "    4.7800291e-02]\n",
            "  [ 1.8995788e-02 -3.7305973e-02 -8.4048510e-04  1.3212610e-02\n",
            "   -2.7593112e-02]\n",
            "  [-1.6415857e-02 -4.8932027e-02 -1.0437034e-02 -3.0794609e-02\n",
            "   -1.1392869e-02]]\n",
            "\n",
            " [[-2.3961747e-02  4.1984547e-02 -1.7944574e-03 -6.9146641e-03\n",
            "   -7.5519085e-04]\n",
            "  [ 4.8937786e-02 -3.5023727e-02  4.1594747e-02  4.5498136e-02\n",
            "   -8.3802566e-03]\n",
            "  [ 1.1336159e-02  4.5345400e-02 -2.1676516e-02 -1.6975045e-02\n",
            "   -2.6233245e-02]\n",
            "  [-3.4260772e-02  4.2987075e-02  1.9957606e-02  5.4235570e-03\n",
            "   -1.4367841e-02]]\n",
            "\n",
            " [[ 1.6098980e-02  1.1575662e-02 -3.7366189e-02  3.4891069e-05\n",
            "    2.0950962e-02]\n",
            "  [ 1.6098980e-02  1.1575662e-02 -3.7366189e-02  3.4891069e-05\n",
            "    2.0950962e-02]\n",
            "  [ 1.2743477e-02 -4.4377577e-02  2.6269604e-02 -3.1500865e-02\n",
            "   -1.0293834e-03]\n",
            "  [ 8.1088915e-03 -4.4455744e-02 -4.8251547e-02  4.4824149e-02\n",
            "    7.5100437e-03]]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hyeonjinho/.pyenv/versions/3.11.6/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 전처리할 텍스트를 정합니다.\n",
        "texts = ['커피 한잔 어때', '오늘 날씨 참 좋네', '옷이 어울려요']\n",
        "\n",
        "# Tokenizer 객체 생성 및 fit_on_texts로 단어 인덱스 학습\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# texts_to_sequences로 텍스트를 시퀀스로 변환\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# 단어 인덱스 확인\n",
        "word_index = tokenizer.word_index\n",
        "print(\"\\n단어 인덱스:\\n\", word_index)\n",
        "\n",
        "# 패딩을 통해 시퀀스 길이를 맞춥니다.\n",
        "print(\"\\n시퀀스:\\n\", sequences)\n",
        "padded_sequences = pad_sequences(sequences, 4)\n",
        "print(\"\\n패딩된 시퀀스:\\n\", padded_sequences)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=5, input_length=4))\n",
        "#input_dim에 1을 더하는 것은 인덱스 0을 패딩 값으로 사용하기 위함.\n",
        "#Keras의 Tokenizer는 단어 인덱스를 1부터 시작하기 때문에, 인덱스 0은 패딩 값으로 예약\n",
        "#output_dim은 단어가 임베딩될 벡터의 길이\n",
        "\n",
        "# 임베딩 결과 확인\n",
        "embedding_output = model.predict(padded_sequences)\n",
        "print(\"\\n임베딩 결과:\\n\", embedding_output)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ch11-colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
