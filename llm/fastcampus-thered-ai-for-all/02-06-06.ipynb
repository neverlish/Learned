{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0_caBv3uJCn"
      },
      "source": [
        "# 6교시 2. 트랜스포머의 기초"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPuREiOFCNFV"
      },
      "source": [
        "입력 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_EGXLJD-CNFV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'data' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Dropout, Add, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 깃허브에 준비된 데이터를 가져옵니다.\n",
        "!git clone https://github.com/taehojo/data.git\n",
        "\n",
        "# CSV 파일 로드\n",
        "dataframe = pd.read_csv('./data/sentiment_data.csv')\n",
        "\n",
        "# 데이터와 라벨 추출\n",
        "sentences = dataframe['sentence'].tolist()\n",
        "labels = dataframe['label'].tolist()\n",
        "\n",
        "# 임베딩 벡터 크기와 최대 문장 길이 설정\n",
        "embedding_dim = 128\n",
        "max_len = 10\n",
        "\n",
        "# 토크나이저 초기화 및 텍스트를 시퀀스로 변환\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# 패딩을 사용하여 시퀀스 길이를 동일하게 맞춤\n",
        "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# 데이터셋을 훈련 세트와 검증 세트로 분리\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3hmyoVGCNFW"
      },
      "source": [
        "포지셔널 인코딩\n",
        "\n",
        "<img src=\"https://github.com/taehojo/fastcampus_ai/blob/master/data/img/06-05.png?raw=1\" width=\"600\"/> <img src=\"https://github.com/taehojo/fastcampus_ai/blob/master/data/img/06-09.png?raw=1\" width=\"200\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mYpJPldOCNFW"
      },
      "outputs": [],
      "source": [
        "# 포지셔널 인코딩 함수\n",
        "\n",
        "def get_positional_encoding(max_len, d_model):\n",
        "    pos_enc = np.zeros((max_len, d_model)) # 포지셔널 인코딩 배열 초기화. max_len: 최대 시퀀스 길이, d_model:임베딩 벡터의 차원\n",
        "\n",
        "    # 시퀀스의 각 위치에 대해 포지셔널 인코딩 값 계산\n",
        "    for pos in range(max_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model))) # 짝수 인덱스\n",
        "            if i + 1 < d_model:\n",
        "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * (i + 1) / d_model))) # 홀수 인덱스\n",
        "\n",
        "# 포지셔널 인코딩 생성\n",
        "positional_encoding = get_positional_encoding(max_len, embedding_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dALh53vsCNFW"
      },
      "source": [
        "정규화, 잔차 연결\n",
        "\n",
        "<img src=\"https://github.com/taehojo/fastcampus_ai/blob/master/data/img/06-06.png?raw=1\" width=\"500\"/><img src=\"https://github.com/taehojo/fastcampus_ai/blob/master/data/img/06-07.png?raw=1\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9Oj9fuVkCNFW"
      },
      "outputs": [],
      "source": [
        "# 멀티헤드 어텐션 레이어\n",
        "\n",
        "class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, key_dim):\n",
        "        super(MultiHeadSelfAttentionLayer, self).__init__() # 레이어가 생성될 때 한 번 실행.\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim) # 멀티헤드 어텐션 레이어 생성. num_heads: 어텐션 헤드의 수, key_dim은 각 헤드의 차원 수\n",
        "        self.norm = LayerNormalization() #레이어 정규화\n",
        "\n",
        "    def call(self, x):\n",
        "        attn_output = self.mha(query=x, value=x, key=x) # 멀티헤드 어텐션 적용\n",
        "        attn_output = self.norm(attn_output + x) # 잔차 연결 적용\n",
        "        return attn_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZMsLM2oCNFW"
      },
      "source": [
        "## 전체 코드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZDlw6XGCNFX"
      },
      "source": [
        "<img src=\"https://github.com/taehojo/fastcampus_ai/blob/master/data/img/06-08.png?raw=1\" width=\"250\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nevncdxVuJCv",
        "outputId": "2a6d1aaa-e54e-4b96-c4ad-c0302717048d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hyeonjinho/.pyenv/versions/3.11.6/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5021 - loss: 0.9174 - val_accuracy: 0.9175 - val_loss: 0.3087\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9781 - loss: 0.1498 - val_accuracy: 1.0000 - val_loss: 0.0116\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9907 - loss: 0.0580 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9946 - loss: 0.0335 - val_accuracy: 0.9925 - val_loss: 0.0212\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9932 - loss: 0.0363 - val_accuracy: 1.0000 - val_loss: 0.0026\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9990 - loss: 0.0073 - val_accuracy: 0.9975 - val_loss: 0.0034\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9953 - loss: 0.0146 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9979 - loss: 0.0077 - val_accuracy: 1.0000 - val_loss: 2.1594e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 1.3077e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9995 - loss: 0.0016 - val_accuracy: 0.9975 - val_loss: 0.0075\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "Text: I absolutely love this!\n",
            "Prediction: Positive\n",
            "Text: I can't stand this product\n",
            "Prediction: Negative\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Dropout, Add, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# CSV 파일 로드\n",
        "dataframe = pd.read_csv('./data/sentiment_data.csv')\n",
        "\n",
        "# 데이터와 라벨 추출\n",
        "sentences = dataframe['sentence'].tolist()\n",
        "labels = dataframe['label'].tolist()\n",
        "\n",
        "# 임베딩 벡터 크기와 최대 문장 길이 설정\n",
        "embedding_dim = 128\n",
        "max_len = 10\n",
        "\n",
        "# 토크나이저 초기화 및 텍스트를 시퀀스로 변환\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# 패딩을 사용하여 시퀀스 길이를 동일하게 맞춤\n",
        "data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# 데이터셋을 훈련 세트와 검증 세트로 분리\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# 포지셔널 인코딩 함수\n",
        "def get_positional_encoding(max_len, d_model):\n",
        "    pos_enc = np.zeros((max_len, d_model)) # 포지셔널 인코딩 배열 초기화. max_len: 최대 시퀀스 길이, d_model:임베딩 벡터의 차원\n",
        "\n",
        "    # 시퀀스의 각 위치에 대해 포지셔널 인코딩 값 계산\n",
        "    for pos in range(max_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model))) # 짝수 인덱스\n",
        "            if i + 1 < d_model:\n",
        "                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * (i + 1) / d_model))) # 홀수 인덱스\n",
        "    return pos_enc  # 포지셔널 인코딩 값을 반환\n",
        "# 포지셔널 인코딩 생성\n",
        "positional_encoding = get_positional_encoding(max_len, embedding_dim)\n",
        "\n",
        "# 멀티헤드 어텐션 레이어\n",
        "class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, key_dim):\n",
        "        super(MultiHeadSelfAttentionLayer, self).__init__() # 레이어가 생성될 때 한 번 실행.\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim) # 멀티헤드 어텐션 레이어 생성. num_heads: 어텐션 헤드의 수, key_dim은 각 헤드의 차원 수\n",
        "        self.norm = LayerNormalization() #레이어 정규화\n",
        "\n",
        "    def call(self, x):\n",
        "        attn_output = self.mha(query=x, value=x, key=x) # 멀티헤드 어텐션 적용\n",
        "        attn_output = self.norm(attn_output + x) # 잔차 연결 적용 후 레이어 정규화\n",
        "        return attn_output\n",
        "\n",
        "# 모델 설정\n",
        "inputs = Input(shape=(max_len,))\n",
        "\n",
        "# 1. 임베딩 레이어: 텍스트 데이터를 임베딩 벡터로 변환합니다.\n",
        "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_len)\n",
        "embedded_sequences = embedding_layer(inputs)\n",
        "\n",
        "# 2. 포지셔널 인코딩 추가\n",
        "embedded_sequences_with_positional_encoding = embedded_sequences + positional_encoding\n",
        "\n",
        "# 3. 멀티헤드 어텐션 레이어 추가\n",
        "attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim)\n",
        "attention_output = attention_layer(embedded_sequences_with_positional_encoding)\n",
        "\n",
        "# 4. 잔차 연결\n",
        "attention_output_with_residual = Add()([embedded_sequences_with_positional_encoding, attention_output])\n",
        "\n",
        "# 5. GlobalAveragePooling1D 레이어 추가 (시퀀스의 전체 정보를 요약하여 다음 레이어로 전달)\n",
        "pooled_output = GlobalAveragePooling1D()(attention_output_with_residual)\n",
        "\n",
        "# 6. 피드 포워드 신경망\n",
        "dense_layer = Dense(128, activation='relu')(pooled_output)\n",
        "dropout_layer = Dropout(0.5)(dense_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
        "\n",
        "# 모델 생성\n",
        "model = Model(inputs=inputs, outputs=output_layer)\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(X_train, np.array(y_train), epochs=10, batch_size=16, validation_data=(X_val, np.array(y_val)))\n",
        "\n",
        "# 샘플 데이터 예측\n",
        "sample_texts = [\"I absolutely love this!\", \"I can't stand this product\"]\n",
        "sample_sequences = tokenizer.texts_to_sequences(sample_texts)\n",
        "sample_data = tf.keras.preprocessing.sequence.pad_sequences(sample_sequences, maxlen=max_len, padding='post')\n",
        "predictions = model.predict(sample_data)\n",
        "\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Prediction: {'Positive' if predictions[i] > 0.5 else 'Negative'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXXX-PKoCNFX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "14-colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
